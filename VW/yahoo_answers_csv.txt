Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        9       13
0.750000 0.750000           16           16.0        7        9       37
0.781250 0.812500           32           32.0        1        9       10
0.843750 0.906250           64           64.0        3        9       10
0.882812 0.921875          128          128.0       10        9      198
0.855469 0.828125          256          256.0        5        9      124
0.832031 0.808594          512          512.0        4       10        9
0.822266 0.812500         1024         1024.0        7        9       49
0.776855 0.731445         2048         2048.0        7       10        8
0.732422 0.687988         4096         4096.0        3       10       54
0.698120 0.663818         8192         8192.0        7        9       15
0.687317 0.676514        16384        16384.0        6       10       47
0.671600 0.655884        32768        32768.0        2        3      281
0.646606 0.621613        65536        65536.0        4       10       52
0.627594 0.608582       131072       131072.0        2       10       30
0.611015 0.594437       262144       262144.0        6       10       35
0.596081 0.581146       524288       524288.0        2       10       67
0.579748 0.579748      1048576      1048576.0        7        7       97 h
0.562413 0.545077      2097152      2097152.0        2        9       79 h
0.541692 0.520970      4194304      4194304.0       10       10      224 h
0.515427 0.489161      8388608      8388608.0        8        3       43 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.446823 h
total feature number = 996468150
Run:{1,None,0.001}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        9       13
0.750000 0.750000           16           16.0        7        9       37
0.781250 0.812500           32           32.0        1        9       10
0.843750 0.906250           64           64.0        3        9       10
0.875000 0.906250          128          128.0       10       10      198
0.832031 0.789062          256          256.0        5        9      124
0.796875 0.761719          512          512.0        4       10        9
0.772461 0.748047         1024         1024.0        7        3       49
0.725586 0.678711         2048         2048.0        7       10        8
0.685303 0.645020         4096         4096.0        3       10       54
0.643555 0.601807         8192         8192.0        7        9       15
0.623779 0.604004        16384        16384.0        6       10       47
0.585663 0.547546        32768        32768.0        2        3      281
0.546600 0.507538        65536        65536.0        4        1       52
0.509056 0.471512       131072       131072.0        2        2       30
0.472443 0.435829       262144       262144.0        6       10       35
0.438675 0.404907       524288       524288.0        2       10       67
0.410072 0.410072      1048576      1048576.0        7        7       97 h
0.386988 0.363904      2097152      2097152.0        2        4       79 h
0.367545 0.348101      4194304      4194304.0       10       10      224 h
0.351015 0.334485      8388608      8388608.0        8        3       43 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.321945 h
total feature number = 996468150
Run:{1,None,0.01}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        9       13
0.750000 0.750000           16           16.0        7        9       37
0.781250 0.812500           32           32.0        1        9       10
0.843750 0.906250           64           64.0        3        9       10
0.843750 0.843750          128          128.0       10       10      198
0.804688 0.765625          256          256.0        5        7      124
0.744141 0.683594          512          512.0        4       10        9
0.685547 0.626953         1024         1024.0        7        1       49
0.618652 0.551758         2048         2048.0        7       10        8
0.546387 0.474121         4096         4096.0        3       10       54
0.485718 0.425049         8192         8192.0        7        8       15
0.442871 0.400024        16384        16384.0        6        6       47
0.406189 0.369507        32768        32768.0        2        2      281
0.380798 0.355408        65536        65536.0        4        4       52
0.359306 0.337814       131072       131072.0        2        2       30
0.341576 0.323845       262144       262144.0        6       10       35
0.327202 0.312828       524288       524288.0        2        4       67
0.314763 0.314763      1048576      1048576.0        7        7       97 h
0.305593 0.296424      2097152      2097152.0        2        4       79 h
0.298600 0.291607      4194304      4194304.0       10       10      224 h
0.293085 0.287570      8388608      8388608.0        8        8       43 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.285373 h
total feature number = 996468150
Run:{1,None,0.1}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        9       13
0.750000 0.750000           16           16.0        7        9       37
0.781250 0.812500           32           32.0        1        3       10
0.828125 0.875000           64           64.0        3        9       10
0.851562 0.875000          128          128.0       10       10      198
0.796875 0.742188          256          256.0        5        5      124
0.701172 0.605469          512          512.0        4       10        9
0.635742 0.570312         1024         1024.0        7        1       49
0.558594 0.481445         2048         2048.0        7        7        8
0.495605 0.432617         4096         4096.0        3       10       54
0.446289 0.396973         8192         8192.0        7        8       15
0.410522 0.374756        16384        16384.0        6        6       47
0.377777 0.345032        32768        32768.0        2        2      281
0.355927 0.334076        65536        65536.0        4        4       52
0.337021 0.318115       131072       131072.0        2        2       30
0.321312 0.305603       262144       262144.0        6        7       35
0.309326 0.297340       524288       524288.0        2        4       67
0.299350 0.299350      1048576      1048576.0        7        7       97 h
0.292719 0.286088      2097152      2097152.0        2        4       79 h
0.289119 0.285518      4194304      4194304.0       10       10      224 h

finished run
number of examples per pass = 933334
passes used = 8
weighted example sum = 7466672.000000
weighted label sum = 0.000000
average loss = 0.285063 h
total feature number = 531449680
Run:{1,None,0.3}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        9       13
0.750000 0.750000           16           16.0        7        9       37
0.781250 0.812500           32           32.0        1        3       10
0.843750 0.906250           64           64.0        3        9       10
0.843750 0.843750          128          128.0       10       10      198
0.781250 0.718750          256          256.0        5        5      124
0.693359 0.605469          512          512.0        4       10        9
0.628906 0.564453         1024         1024.0        7        1       49
0.550293 0.471680         2048         2048.0        7        7        8
0.485840 0.421387         4096         4096.0        3       10       54
0.438354 0.390869         8192         8192.0        7        8       15
0.404358 0.370361        16384        16384.0        6        6       47
0.372589 0.340820        32768        32768.0        2        2      281
0.351974 0.331360        65536        65536.0        4        4       52
0.334061 0.316147       131072       131072.0        2        2       30
0.319511 0.304962       262144       262144.0        6        7       35
0.307810 0.296108       524288       524288.0        2        4       67
0.298068 0.298068      1048576      1048576.0        7        7       97 h
0.292340 0.286612      2097152      2097152.0        2        4       79 h
0.289978 0.287615      4194304      4194304.0       10       10      224 h

finished run
number of examples per pass = 933334
passes used = 6
weighted example sum = 5600004.000000
weighted label sum = 0.000000
average loss = 0.286635 h
total feature number = 398587260
Run:{1,None,0.5}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        4       13
0.750000 0.750000           16           16.0        7        9       37
0.812500 0.875000           32           32.0        1        3       10
0.875000 0.937500           64           64.0        3        9       10
0.843750 0.812500          128          128.0       10       10      198
0.792969 0.742188          256          256.0        5        5      124
0.697266 0.601562          512          512.0        4       10        9
0.629883 0.562500         1024         1024.0        7        1       49
0.547852 0.465820         2048         2048.0        7        7        8
0.482178 0.416504         4096         4096.0        3       10       54
0.438599 0.395020         8192         8192.0        7        8       15
0.404907 0.371216        16384        16384.0        6        6       47
0.373077 0.341248        32768        32768.0        2        2      281
0.352646 0.332214        65536        65536.0        4        4       52
0.334793 0.316940       131072       131072.0        2        2       30
0.320847 0.306900       262144       262144.0        6        7       35
0.309078 0.297310       524288       524288.0        2        4       67
0.299131 0.299131      1048576      1048576.0        7        7       97 h
0.293833 0.288535      2097152      2097152.0        2        4       79 h
0.292347 0.290861      4194304      4194304.0       10       10      224 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.288551 h
total feature number = 332156050
Run:{1,None,0.7}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        4       13
0.750000 0.750000           16           16.0        7        9       37
0.812500 0.875000           32           32.0        1        3       10
0.875000 0.937500           64           64.0        3        9       10
0.843750 0.812500          128          128.0       10       10      198
0.789062 0.734375          256          256.0        5        5      124
0.695312 0.601562          512          512.0        4       10        9
0.625000 0.554688         1024         1024.0        7        1       49
0.542480 0.459961         2048         2048.0        7        7        8
0.476807 0.411133         4096         4096.0        3       10       54
0.435425 0.394043         8192         8192.0        7        8       15
0.402771 0.370117        16384        16384.0        6        6       47
0.373505 0.344238        32768        32768.0        2        2      281
0.354019 0.334534        65536        65536.0        4        4       52
0.336700 0.319382       131072       131072.0        2        2       30
0.322910 0.309120       262144       262144.0        6       10       35
0.311243 0.299576       524288       524288.0        2        4       67
0.301521 0.301521      1048576      1048576.0        7        7       97 h
0.296303 0.291085      2097152      2097152.0        2        4       79 h
0.295209 0.294116      4194304      4194304.0       10       10      224 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.290960 h
total feature number = 332156050
Run:{1,None,0.9}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        9       13
0.750000 0.750000           16           16.0        7        9       37
0.781250 0.812500           32           32.0        1        9       10
0.843750 0.906250           64           64.0        3        9       10
0.882812 0.921875          128          128.0       10        9      198
0.855469 0.828125          256          256.0        5        9      124
0.832031 0.808594          512          512.0        4       10        9
0.822266 0.812500         1024         1024.0        7        9       49
0.776855 0.731445         2048         2048.0        7       10        8
0.732422 0.687988         4096         4096.0        3       10       54
0.698120 0.663818         8192         8192.0        7        9       15
0.687317 0.676514        16384        16384.0        6       10       47
0.671600 0.655884        32768        32768.0        2        3      281
0.646606 0.621613        65536        65536.0        4       10       52
0.627594 0.608582       131072       131072.0        2       10       30
0.611015 0.594437       262144       262144.0        6       10       35
0.596081 0.581146       524288       524288.0        2       10       67
0.579748 0.579748      1048576      1048576.0        7        7       97 h
0.562413 0.545077      2097152      2097152.0        2        9       79 h
0.541692 0.520970      4194304      4194304.0       10       10      224 h
0.515427 0.489161      8388608      8388608.0        8        3       43 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.446823 h
total feature number = 996468150
Run:{1,1,0.001}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        9       13
0.750000 0.750000           16           16.0        7        9       37
0.781250 0.812500           32           32.0        1        9       10
0.843750 0.906250           64           64.0        3        9       10
0.875000 0.906250          128          128.0       10       10      198
0.832031 0.789062          256          256.0        5        9      124
0.796875 0.761719          512          512.0        4       10        9
0.772461 0.748047         1024         1024.0        7        3       49
0.725586 0.678711         2048         2048.0        7       10        8
0.685303 0.645020         4096         4096.0        3       10       54
0.643555 0.601807         8192         8192.0        7        9       15
0.623779 0.604004        16384        16384.0        6       10       47
0.585663 0.547546        32768        32768.0        2        3      281
0.546600 0.507538        65536        65536.0        4        1       52
0.509056 0.471512       131072       131072.0        2        2       30
0.472443 0.435829       262144       262144.0        6       10       35
0.438675 0.404907       524288       524288.0        2       10       67
0.410072 0.410072      1048576      1048576.0        7        7       97 h
0.386988 0.363904      2097152      2097152.0        2        4       79 h
0.367545 0.348101      4194304      4194304.0       10       10      224 h
0.351015 0.334485      8388608      8388608.0        8        3       43 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.321945 h
total feature number = 996468150
Run:{1,1,0.01}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        9       13
0.750000 0.750000           16           16.0        7        9       37
0.781250 0.812500           32           32.0        1        9       10
0.843750 0.906250           64           64.0        3        9       10
0.843750 0.843750          128          128.0       10       10      198
0.804688 0.765625          256          256.0        5        7      124
0.744141 0.683594          512          512.0        4       10        9
0.685547 0.626953         1024         1024.0        7        1       49
0.618652 0.551758         2048         2048.0        7       10        8
0.546387 0.474121         4096         4096.0        3       10       54
0.485718 0.425049         8192         8192.0        7        8       15
0.442871 0.400024        16384        16384.0        6        6       47
0.406189 0.369507        32768        32768.0        2        2      281
0.380798 0.355408        65536        65536.0        4        4       52
0.359306 0.337814       131072       131072.0        2        2       30
0.341576 0.323845       262144       262144.0        6       10       35
0.327202 0.312828       524288       524288.0        2        4       67
0.314763 0.314763      1048576      1048576.0        7        7       97 h
0.305593 0.296424      2097152      2097152.0        2        4       79 h
0.298600 0.291607      4194304      4194304.0       10       10      224 h
0.293085 0.287570      8388608      8388608.0        8        8       43 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.285373 h
total feature number = 996468150
Run:{1,1,0.1}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        9       13
0.750000 0.750000           16           16.0        7        9       37
0.781250 0.812500           32           32.0        1        3       10
0.828125 0.875000           64           64.0        3        9       10
0.851562 0.875000          128          128.0       10       10      198
0.796875 0.742188          256          256.0        5        5      124
0.701172 0.605469          512          512.0        4       10        9
0.635742 0.570312         1024         1024.0        7        1       49
0.558594 0.481445         2048         2048.0        7        7        8
0.495605 0.432617         4096         4096.0        3       10       54
0.446289 0.396973         8192         8192.0        7        8       15
0.410522 0.374756        16384        16384.0        6        6       47
0.377777 0.345032        32768        32768.0        2        2      281
0.355927 0.334076        65536        65536.0        4        4       52
0.337021 0.318115       131072       131072.0        2        2       30
0.321312 0.305603       262144       262144.0        6        7       35
0.309326 0.297340       524288       524288.0        2        4       67
0.299350 0.299350      1048576      1048576.0        7        7       97 h
0.292719 0.286088      2097152      2097152.0        2        4       79 h
0.289119 0.285518      4194304      4194304.0       10       10      224 h

finished run
number of examples per pass = 933334
passes used = 8
weighted example sum = 7466672.000000
weighted label sum = 0.000000
average loss = 0.285063 h
total feature number = 531449680
Run:{1,1,0.3}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        9       13
0.750000 0.750000           16           16.0        7        9       37
0.781250 0.812500           32           32.0        1        3       10
0.843750 0.906250           64           64.0        3        9       10
0.843750 0.843750          128          128.0       10       10      198
0.781250 0.718750          256          256.0        5        5      124
0.693359 0.605469          512          512.0        4       10        9
0.628906 0.564453         1024         1024.0        7        1       49
0.550293 0.471680         2048         2048.0        7        7        8
0.485840 0.421387         4096         4096.0        3       10       54
0.438354 0.390869         8192         8192.0        7        8       15
0.404358 0.370361        16384        16384.0        6        6       47
0.372589 0.340820        32768        32768.0        2        2      281
0.351974 0.331360        65536        65536.0        4        4       52
0.334061 0.316147       131072       131072.0        2        2       30
0.319511 0.304962       262144       262144.0        6        7       35
0.307810 0.296108       524288       524288.0        2        4       67
0.298068 0.298068      1048576      1048576.0        7        7       97 h
0.292340 0.286612      2097152      2097152.0        2        4       79 h
0.289978 0.287615      4194304      4194304.0       10       10      224 h

finished run
number of examples per pass = 933334
passes used = 6
weighted example sum = 5600004.000000
weighted label sum = 0.000000
average loss = 0.286635 h
total feature number = 398587260
Run:{1,1,0.5}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        4       13
0.750000 0.750000           16           16.0        7        9       37
0.812500 0.875000           32           32.0        1        3       10
0.875000 0.937500           64           64.0        3        9       10
0.843750 0.812500          128          128.0       10       10      198
0.792969 0.742188          256          256.0        5        5      124
0.697266 0.601562          512          512.0        4       10        9
0.629883 0.562500         1024         1024.0        7        1       49
0.547852 0.465820         2048         2048.0        7        7        8
0.482178 0.416504         4096         4096.0        3       10       54
0.438599 0.395020         8192         8192.0        7        8       15
0.404907 0.371216        16384        16384.0        6        6       47
0.373077 0.341248        32768        32768.0        2        2      281
0.352646 0.332214        65536        65536.0        4        4       52
0.334793 0.316940       131072       131072.0        2        2       30
0.320847 0.306900       262144       262144.0        6        7       35
0.309078 0.297310       524288       524288.0        2        4       67
0.299131 0.299131      1048576      1048576.0        7        7       97 h
0.293833 0.288535      2097152      2097152.0        2        4       79 h
0.292347 0.290861      4194304      4194304.0       10       10      224 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.288551 h
total feature number = 332156050
Run:{1,1,0.7}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      192
1.000000 1.000000            2            2.0        4        9       10
0.500000 0.000000            4            4.0        9        9       91
0.750000 1.000000            8            8.0       10        4       13
0.750000 0.750000           16           16.0        7        9       37
0.812500 0.875000           32           32.0        1        3       10
0.875000 0.937500           64           64.0        3        9       10
0.843750 0.812500          128          128.0       10       10      198
0.789062 0.734375          256          256.0        5        5      124
0.695312 0.601562          512          512.0        4       10        9
0.625000 0.554688         1024         1024.0        7        1       49
0.542480 0.459961         2048         2048.0        7        7        8
0.476807 0.411133         4096         4096.0        3       10       54
0.435425 0.394043         8192         8192.0        7        8       15
0.402771 0.370117        16384        16384.0        6        6       47
0.373505 0.344238        32768        32768.0        2        2      281
0.354019 0.334534        65536        65536.0        4        4       52
0.336700 0.319382       131072       131072.0        2        2       30
0.322910 0.309120       262144       262144.0        6       10       35
0.311243 0.299576       524288       524288.0        2        4       67
0.301521 0.301521      1048576      1048576.0        7        7       97 h
0.296303 0.291085      2097152      2097152.0        2        4       79 h
0.295209 0.294116      4194304      4194304.0       10       10      224 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.290960 h
total feature number = 332156050
Run:{1,1,0.9}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      382
1.000000 1.000000            2            2.0        4        9       18
0.500000 0.000000            4            4.0        9        9      180
0.750000 1.000000            8            8.0       10        9       24
0.750000 0.750000           16           16.0        7        9       72
0.781250 0.812500           32           32.0        1        9       18
0.843750 0.906250           64           64.0        3        9       18
0.882812 0.921875          128          128.0       10        9      394
0.851562 0.820312          256          256.0        5        9      246
0.826172 0.800781          512          512.0        4       10       16
0.809570 0.792969         1024         1024.0        7        9       96
0.763184 0.716797         2048         2048.0        7       10       14
0.717285 0.671387         4096         4096.0        3       10      106
0.685059 0.652832         8192         8192.0        7        9       28
0.671509 0.657959        16384        16384.0        6       10       92
0.655396 0.639282        32768        32768.0        2        3      560
0.630539 0.605682        65536        65536.0        4        1      102
0.610809 0.591080       131072       131072.0        2       10       58
0.594685 0.578560       262144       262144.0        6       10       68
0.580473 0.566261       524288       524288.0        2       10      132
0.565010 0.565010      1048576      1048576.0        7        7      192 h
0.549502 0.533994      2097152      2097152.0        2        9      156 h
0.531468 0.513434      4194304      4194304.0       10       10      446 h
0.508943 0.486419      8388608      8388608.0        8        3       84 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.448177 h
total feature number = 1964936385
Run:{2,None,0.001}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      382
1.000000 1.000000            2            2.0        4        9       18
0.500000 0.000000            4            4.0        9        9      180
0.750000 1.000000            8            8.0       10        9       24
0.750000 0.750000           16           16.0        7        9       72
0.781250 0.812500           32           32.0        1        9       18
0.843750 0.906250           64           64.0        3        9       18
0.875000 0.906250          128          128.0       10       10      394
0.828125 0.781250          256          256.0        5        9      246
0.791016 0.753906          512          512.0        4       10       16
0.764648 0.738281         1024         1024.0        7        3       96
0.718262 0.671875         2048         2048.0        7       10       14
0.681152 0.644043         4096         4096.0        3       10      106
0.639160 0.597168         8192         8192.0        7        9       28
0.618774 0.598389        16384        16384.0        6       10       92
0.583221 0.547668        32768        32768.0        2        3      560
0.546371 0.509521        65536        65536.0        4        1      102
0.512665 0.478958       131072       131072.0        2        2       58
0.478653 0.444641       262144       262144.0        6       10       68
0.444313 0.409973       524288       524288.0        2       10      132
0.413694 0.413694      1048576      1048576.0        7        7      192 h
0.389301 0.364909      2097152      2097152.0        2        4      156 h
0.368770 0.348238      4194304      4194304.0       10       10      446 h
0.351273 0.333777      8388608      8388608.0        8        3       84 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.319593 h
total feature number = 1964936385
Run:{2,None,0.01}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      382
1.000000 1.000000            2            2.0        4        9       18
0.500000 0.000000            4            4.0        9        9      180
0.750000 1.000000            8            8.0       10        9       24
0.750000 0.750000           16           16.0        7        9       72
0.781250 0.812500           32           32.0        1        9       18
0.843750 0.906250           64           64.0        3        9       18
0.843750 0.843750          128          128.0       10       10      394
0.812500 0.781250          256          256.0        5       10      246
0.750000 0.687500          512          512.0        4       10       16
0.696289 0.642578         1024         1024.0        7        3       96
0.629395 0.562500         2048         2048.0        7       10       14
0.562744 0.496094         4096         4096.0        3       10      106
0.500366 0.437988         8192         8192.0        7        8       28
0.455750 0.411133        16384        16384.0        6        6       92
0.416321 0.376892        32768        32768.0        2        2      560
0.389206 0.362091        65536        65536.0        4        4      102
0.365150 0.341095       131072       131072.0        2        2       58
0.344574 0.323997       262144       262144.0        6       10       68
0.328342 0.312111       524288       524288.0        2        2      132
0.313960 0.313960      1048576      1048576.0        7        7      192 h
0.303477 0.292994      2097152      2097152.0        2        4      156 h
0.295658 0.287839      4194304      4194304.0       10       10      446 h
0.289799 0.283940      8388608      8388608.0        8        8       84 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.282461 h
total feature number = 1964936385
Run:{2,None,0.1}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      382
1.000000 1.000000            2            2.0        4        9       18
0.500000 0.000000            4            4.0        9        9      180
0.750000 1.000000            8            8.0       10        9       24
0.750000 0.750000           16           16.0        7        9       72
0.781250 0.812500           32           32.0        1        4       18
0.828125 0.875000           64           64.0        3        9       18
0.828125 0.828125          128          128.0       10       10      394
0.796875 0.765625          256          256.0        5        7      246
0.705078 0.613281          512          512.0        4       10       16
0.634766 0.564453         1024         1024.0        7        1       96
0.564453 0.494141         2048         2048.0        7        7       14
0.498291 0.432129         4096         4096.0        3       10      106
0.453003 0.407715         8192         8192.0        7        8       28
0.414307 0.375610        16384        16384.0        6        6       92
0.383759 0.353210        32768        32768.0        2        2      560
0.360168 0.336578        65536        65536.0        4        4      102
0.340538 0.320908       131072       131072.0        2        2       58
0.323612 0.306686       262144       262144.0        6        7       68
0.309746 0.295879       524288       524288.0        2        2      132
0.298124 0.298124      1048576      1048576.0        7        7      192 h
0.290766 0.283408      2097152      2097152.0        2        4      156 h
0.286966 0.283166      4194304      4194304.0       10       10      446 h

finished run
number of examples per pass = 933334
passes used = 6
weighted example sum = 5600004.000000
weighted label sum = 0.000000
average loss = 0.282795 h
total feature number = 785974554
Run:{2,None,0.3}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      382
1.000000 1.000000            2            2.0        4        9       18
0.500000 0.000000            4            4.0        9        9      180
0.750000 1.000000            8            8.0       10        9       24
0.750000 0.750000           16           16.0        7        9       72
0.781250 0.812500           32           32.0        1        4       18
0.843750 0.906250           64           64.0        3        9       18
0.812500 0.781250          128          128.0       10       10      394
0.769531 0.726562          256          256.0        5        5      246
0.679688 0.589844          512          512.0        4       10       16
0.621094 0.562500         1024         1024.0        7        1       96
0.559570 0.498047         2048         2048.0        7        7       14
0.495117 0.430664         4096         4096.0        3       10      106
0.447632 0.400146         8192         8192.0        7        8       28
0.409241 0.370850        16384        16384.0        6        6       92
0.379028 0.348816        32768        32768.0        2        2      560
0.355698 0.332367        65536        65536.0        4        4      102
0.336327 0.316956       131072       131072.0        2        2       58
0.320419 0.304512       262144       262144.0        6        7       68
0.307474 0.294529       524288       524288.0        2        2      132
0.297111 0.297111      1048576      1048576.0        7        7      192 h
0.290820 0.284529      2097152      2097152.0        2        4      156 h
0.288683 0.286546      4194304      4194304.0       10       10      446 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.284960 h
total feature number = 654978795
Run:{2,None,0.5}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      382
1.000000 1.000000            2            2.0        4        9       18
0.500000 0.000000            4            4.0        9        9      180
0.750000 1.000000            8            8.0       10        4       24
0.750000 0.750000           16           16.0        7        9       72
0.812500 0.875000           32           32.0        1        4       18
0.859375 0.906250           64           64.0        3        9       18
0.820312 0.781250          128          128.0       10       10      394
0.757812 0.695312          256          256.0        5        5      246
0.671875 0.585938          512          512.0        4       10       16
0.615234 0.558594         1024         1024.0        7        1       96
0.555176 0.495117         2048         2048.0        7        7       14
0.491211 0.427246         4096         4096.0        3       10      106
0.445068 0.398926         8192         8192.0        7        8       28
0.406982 0.368896        16384        16384.0        6        8       92
0.376343 0.345703        32768        32768.0        2        2      560
0.354294 0.332245        65536        65536.0        4        4      102
0.335526 0.316757       131072       131072.0        2        2       58
0.320271 0.305016       262144       262144.0        6        7       68
0.308023 0.295776       524288       524288.0        2        2      132
0.298326 0.298326      1048576      1048576.0        7        7      192 h
0.292773 0.287221      2097152      2097152.0        2        2      156 h
0.291275 0.289776      4194304      4194304.0       10       10      446 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.287383 h
total feature number = 654978795
Run:{2,None,0.7}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      382
1.000000 1.000000            2            2.0        4        9       18
0.500000 0.000000            4            4.0        9        9      180
0.750000 1.000000            8            8.0       10        4       24
0.750000 0.750000           16           16.0        7        9       72
0.812500 0.875000           32           32.0        1        4       18
0.859375 0.906250           64           64.0        3        9       18
0.820312 0.781250          128          128.0       10       10      394
0.753906 0.687500          256          256.0        5        5      246
0.675781 0.597656          512          512.0        4       10       16
0.619141 0.562500         1024         1024.0        7        1       96
0.553711 0.488281         2048         2048.0        7        7       14
0.488281 0.422852         4096         4096.0        3       10      106
0.442627 0.396973         8192         8192.0        7        8       28
0.405762 0.368896        16384        16384.0        6        8       92
0.374725 0.343689        32768        32768.0        2        2      560
0.353531 0.332336        65536        65536.0        4        4      102
0.335342 0.317154       131072       131072.0        2        2       58
0.320545 0.305748       262144       262144.0        6        7       68
0.309057 0.297569       524288       524288.0        2        2      132
0.300214 0.300214      1048576      1048576.0        7        7      192 h
0.295182 0.290150      2097152      2097152.0        2        2      156 h
0.293967 0.292752      4194304      4194304.0       10       10      446 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.290291 h
total feature number = 654978795
Run:{2,None,0.9}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        9       34
0.750000 0.750000           16           16.0        7        9      106
0.781250 0.812500           32           32.0        1        9       25
0.843750 0.906250           64           64.0        3        9       25
0.882812 0.921875          128          128.0       10        9      589
0.851562 0.820312          256          256.0        5        9      367
0.830078 0.808594          512          512.0        4       10       22
0.814453 0.798828         1024         1024.0        7        9      142
0.766113 0.717773         2048         2048.0        7       10       19
0.717529 0.668945         4096         4096.0        3       10      157
0.682007 0.646484         8192         8192.0        7        9       40
0.666199 0.650391        16384        16384.0        6       10      136
0.647827 0.629456        32768        32768.0        2        3      838
0.623322 0.598816        65536        65536.0        4        1      151
0.603615 0.583908       131072       131072.0        2       10       85
0.587727 0.571838       262144       262144.0        6       10      100
0.574511 0.561295       524288       524288.0        2       10      196
0.559778 0.559778      1048576      1048576.0        7        7      286 h
0.544862 0.529946      2097152      2097152.0        2        9      232 h
0.528009 0.511156      4194304      4194304.0       10       10      667 h
0.507112 0.486216      8388608      8388608.0        8        3      124 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.450984 h
total feature number = 2919406050
Run:{2,1,0.001}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        9       34
0.750000 0.750000           16           16.0        7        9      106
0.781250 0.812500           32           32.0        1        9       25
0.843750 0.906250           64           64.0        3        9       25
0.875000 0.906250          128          128.0       10       10      589
0.835938 0.796875          256          256.0        5        9      367
0.792969 0.750000          512          512.0        4       10       22
0.765625 0.738281         1024         1024.0        7        3      142
0.719238 0.672852         2048         2048.0        7       10       19
0.674072 0.628906         4096         4096.0        3       10      157
0.636597 0.599121         8192         8192.0        7        9       40
0.616882 0.597168        16384        16384.0        6       10      136
0.582611 0.548340        32768        32768.0        2        3      838
0.546539 0.510468        65536        65536.0        4        1      151
0.514084 0.481628       131072       131072.0        2        2       85
0.481945 0.449806       262144       262144.0        6       10      100
0.448153 0.414360       524288       524288.0        2       10      196
0.417047 0.417047      1048576      1048576.0        7        7      286 h
0.392078 0.367108      2097152      2097152.0        2        4      232 h
0.371264 0.350451      4194304      4194304.0       10       10      667 h
0.353496 0.335728      8388608      8388608.0        8        8      124 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.321808 h
total feature number = 2919406050
Run:{2,1,0.01}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        9       34
0.750000 0.750000           16           16.0        7        9      106
0.781250 0.812500           32           32.0        1        9       25
0.843750 0.906250           64           64.0        3        9       25
0.843750 0.843750          128          128.0       10       10      589
0.816406 0.789062          256          256.0        5       10      367
0.761719 0.707031          512          512.0        4       10       22
0.705078 0.648438         1024         1024.0        7        3      142
0.636719 0.568359         2048         2048.0        7       10       19
0.571533 0.506348         4096         4096.0        3       10      157
0.512573 0.453613         8192         8192.0        7        8       40
0.465149 0.417725        16384        16384.0        6        6      136
0.422546 0.379944        32768        32768.0        2        3      838
0.393356 0.364166        65536        65536.0        4        4      151
0.368767 0.344177       131072       131072.0        2        2       85
0.347805 0.326843       262144       262144.0        6       10      100
0.331148 0.314491       524288       524288.0        2        4      196
0.316695 0.316695      1048576      1048576.0        7        7      286 h
0.305894 0.295092      2097152      2097152.0        2        4      232 h
0.297808 0.289723      4194304      4194304.0       10       10      667 h
0.291822 0.285835      8388608      8388608.0        8        8      124 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.284409 h
total feature number = 2919406050
Run:{2,1,0.1}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        9       34
0.750000 0.750000           16           16.0        7        9      106
0.781250 0.812500           32           32.0        1        4       25
0.843750 0.906250           64           64.0        3        9       25
0.820312 0.796875          128          128.0       10       10      589
0.781250 0.742188          256          256.0        5       10      367
0.720703 0.660156          512          512.0        4       10       22
0.653320 0.585938         1024         1024.0        7        3      142
0.583008 0.512695         2048         2048.0        7       10       19
0.511230 0.439453         4096         4096.0        3       10      157
0.462891 0.414551         8192         8192.0        7        8       40
0.423767 0.384644        16384        16384.0        6        8      136
0.389862 0.355957        32768        32768.0        2        2      838
0.366272 0.342682        65536        65536.0        4        4      151
0.344612 0.322952       131072       131072.0        2        2       85
0.327095 0.309578       262144       262144.0        6       10      100
0.313053 0.299011       524288       524288.0        2        4      196
0.300985 0.300985      1048576      1048576.0        7        7      286 h
0.293511 0.286038      2097152      2097152.0        2        4      232 h
0.289443 0.285375      4194304      4194304.0       10       10      667 h

finished run
number of examples per pass = 933334
passes used = 7
weighted example sum = 6533338.000000
weighted label sum = 0.000000
average loss = 0.285150 h
total feature number = 1362389490
Run:{2,1,0.3}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        9       34
0.750000 0.750000           16           16.0        7        9      106
0.781250 0.812500           32           32.0        1        4       25
0.828125 0.875000           64           64.0        3        9       25
0.812500 0.796875          128          128.0       10       10      589
0.777344 0.742188          256          256.0        5        5      367
0.701172 0.625000          512          512.0        4       10       22
0.639648 0.578125         1024         1024.0        7        3      142
0.573242 0.506836         2048         2048.0        7        7       19
0.505859 0.438477         4096         4096.0        3       10      157
0.457886 0.409912         8192         8192.0        7        8       40
0.417419 0.376953        16384        16384.0        6        8      136
0.384125 0.350830        32768        32768.0        2        2      838
0.361023 0.337921        65536        65536.0        4        4      151
0.340500 0.319977       131072       131072.0        2        2       85
0.323780 0.307060       262144       262144.0        6       10      100
0.310501 0.297222       524288       524288.0        2        4      196
0.300012 0.300012      1048576      1048576.0        7        7      286 h
0.293803 0.287594      2097152      2097152.0        2        2      232 h
0.291146 0.288490      4194304      4194304.0       10       10      667 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.287604 h
total feature number = 973135350
Run:{2,1,0.5}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        4       34
0.750000 0.750000           16           16.0        7        9      106
0.812500 0.875000           32           32.0        1        4       25
0.859375 0.906250           64           64.0        3        9       25
0.828125 0.796875          128          128.0       10       10      589
0.773438 0.718750          256          256.0        5        5      367
0.695312 0.617188          512          512.0        4       10       22
0.634766 0.574219         1024         1024.0        7        3      142
0.569336 0.503906         2048         2048.0        7        7       19
0.502197 0.435059         4096         4096.0        3       10      157
0.455688 0.409180         8192         8192.0        7        8       40
0.415161 0.374634        16384        16384.0        6        8      136
0.382172 0.349182        32768        32768.0        2        2      838
0.359253 0.336334        65536        65536.0        4        4      151
0.339050 0.318848       131072       131072.0        2        2       85
0.323265 0.307480       262144       262144.0        6        7      100
0.310970 0.298676       524288       524288.0        2        4      196
0.301194 0.301194      1048576      1048576.0        7        7      286 h
0.295755 0.290316      2097152      2097152.0        2        2      232 h
0.293693 0.291630      4194304      4194304.0       10       10      667 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.290300 h
total feature number = 973135350
Run:{2,1,0.7}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        4       34
0.750000 0.750000           16           16.0        7        9      106
0.843750 0.937500           32           32.0        1        4       25
0.875000 0.906250           64           64.0        3        9       25
0.835938 0.796875          128          128.0       10       10      589
0.773438 0.710938          256          256.0        5        5      367
0.697266 0.621094          512          512.0        4       10       22
0.635742 0.574219         1024         1024.0        7        3      142
0.568848 0.501953         2048         2048.0        7        7       19
0.499756 0.430664         4096         4096.0        3       10      157
0.453491 0.407227         8192         8192.0        7        8       40
0.413147 0.372803        16384        16384.0        6        8      136
0.380219 0.347290        32768        32768.0        2        2      838
0.358185 0.336151        65536        65536.0        4        4      151
0.338646 0.319107       131072       131072.0        2        2       85
0.323627 0.308609       262144       262144.0        6        7      100
0.311861 0.300095       524288       524288.0        2        4      196
0.302715 0.302715      1048576      1048576.0        7        7      286 h
0.297667 0.292620      2097152      2097152.0        2        2      232 h
0.295965 0.294263      4194304      4194304.0       10       10      667 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.292528 h
total feature number = 973135350
Run:{2,1,0.9}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        9       34
0.750000 0.750000           16           16.0        7        9      106
0.781250 0.812500           32           32.0        1        9       25
0.843750 0.906250           64           64.0        3        9       25
0.882812 0.921875          128          128.0       10        9      589
0.851562 0.820312          256          256.0        5        9      367
0.830078 0.808594          512          512.0        4       10       22
0.811523 0.792969         1024         1024.0        7        9      142
0.764160 0.716797         2048         2048.0        7       10       19
0.718994 0.673828         4096         4096.0        3       10      157
0.686157 0.653320         8192         8192.0        7        9       40
0.672241 0.658325        16384        16384.0        6       10      136
0.655914 0.639587        32768        32768.0        2        3      838
0.631363 0.606812        65536        65536.0        4       10      151
0.611877 0.592392       131072       131072.0        2       10       85
0.596252 0.580627       262144       262144.0        6       10      100
0.583050 0.569847       524288       524288.0        2       10      196
0.568678 0.568678      1048576      1048576.0        7        7      286 h
0.554516 0.540355      2097152      2097152.0        2        9      232 h
0.538413 0.522310      4194304      4194304.0       10       10      667 h
0.518411 0.498409      8388608      8388608.0        8        9      124 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.463364 h
total feature number = 2919406050
Run:{3,None,0.001}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        9       34
0.750000 0.750000           16           16.0        7        9      106
0.781250 0.812500           32           32.0        1        9       25
0.843750 0.906250           64           64.0        3        9       25
0.875000 0.906250          128          128.0       10       10      589
0.839844 0.804688          256          256.0        5        9      367
0.798828 0.757812          512          512.0        4       10       22
0.772461 0.746094         1024         1024.0        7        3      142
0.723145 0.673828         2048         2048.0        7       10       19
0.683594 0.644043         4096         4096.0        3       10      157
0.644531 0.605469         8192         8192.0        7        9       40
0.625671 0.606812        16384        16384.0        6       10      136
0.592896 0.560120        32768        32768.0        2        3      838
0.557327 0.521759        65536        65536.0        4        1      151
0.526024 0.494720       131072       131072.0        2        2       85
0.494495 0.462967       262144       262144.0        6       10      100
0.459946 0.425396       524288       524288.0        2       10      196
0.427089 0.427089      1048576      1048576.0        7        7      286 h
0.400502 0.373915      2097152      2097152.0        2        4      232 h
0.378509 0.356517      4194304      4194304.0       10       10      667 h
0.359896 0.341282      8388608      8388608.0        8        3      124 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.326263 h
total feature number = 2919406050
Run:{3,None,0.01}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        9       34
0.750000 0.750000           16           16.0        7        9      106
0.781250 0.812500           32           32.0        1        9       25
0.843750 0.906250           64           64.0        3        9       25
0.851562 0.859375          128          128.0       10       10      589
0.820312 0.789062          256          256.0        5       10      367
0.763672 0.707031          512          512.0        4       10       22
0.705078 0.646484         1024         1024.0        7        3      142
0.640137 0.575195         2048         2048.0        7       10       19
0.576660 0.513184         4096         4096.0        3       10      157
0.516602 0.456543         8192         8192.0        7        8       40
0.467834 0.419067        16384        16384.0        6        6      136
0.427673 0.387512        32768        32768.0        2        3      838
0.399536 0.371399        65536        65536.0        4        4      151
0.374718 0.349899       131072       131072.0        2        2       85
0.353428 0.332138       262144       262144.0        6       10      100
0.336435 0.319443       524288       524288.0        2        2      196
0.321227 0.321227      1048576      1048576.0        7        7      286 h
0.310219 0.299210      2097152      2097152.0        2        4      232 h
0.301958 0.293697      4194304      4194304.0       10       10      667 h
0.295946 0.289934      8388608      8388608.0        8        8      124 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.288665 h
total feature number = 2919406050
Run:{3,None,0.1}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        9       34
0.750000 0.750000           16           16.0        7        9      106
0.781250 0.812500           32           32.0        1        4       25
0.843750 0.906250           64           64.0        3        9       25
0.835938 0.828125          128          128.0       10       10      589
0.800781 0.765625          256          256.0        5       10      367
0.716797 0.632812          512          512.0        4       10       22
0.658203 0.599609         1024         1024.0        7        1      142
0.585449 0.512695         2048         2048.0        7        7       19
0.513428 0.441406         4096         4096.0        3       10      157
0.464233 0.415039         8192         8192.0        7        8       40
0.427307 0.390381        16384        16384.0        6        6      136
0.394165 0.361023        32768        32768.0        2        2      838
0.369720 0.345276        65536        65536.0        4        4      151
0.349312 0.328903       131072       131072.0        2        2       85
0.332069 0.314827       262144       262144.0        6        7      100
0.317869 0.303669       524288       524288.0        2        2      196
0.305330 0.305330      1048576      1048576.0        7        7      286 h
0.297852 0.290375      2097152      2097152.0        2        2      232 h
0.293772 0.289692      4194304      4194304.0       10       10      667 h

finished run
number of examples per pass = 933334
passes used = 6
weighted example sum = 5600004.000000
weighted label sum = 0.000000
average loss = 0.289447 h
total feature number = 1167762420
Run:{3,None,0.3}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        4       34
0.750000 0.750000           16           16.0        7        9      106
0.781250 0.812500           32           32.0        1        4       25
0.843750 0.906250           64           64.0        3        9       25
0.820312 0.796875          128          128.0       10       10      589
0.785156 0.750000          256          256.0        5        5      367
0.699219 0.613281          512          512.0        4       10       22
0.637695 0.576172         1024         1024.0        7        1      142
0.569336 0.500977         2048         2048.0        7        7       19
0.502686 0.436035         4096         4096.0        3       10      157
0.455444 0.408203         8192         8192.0        7        8       40
0.418335 0.381226        16384        16384.0        6        6      136
0.386322 0.354309        32768        32768.0        2        2      838
0.363358 0.340393        65536        65536.0        4        4      151
0.344269 0.325180       131072       131072.0        2        2       85
0.328110 0.311951       262144       262144.0        6        7      100
0.315075 0.302040       524288       524288.0        2        2      196
0.304248 0.304248      1048576      1048576.0        7        7      286 h
0.298079 0.291911      2097152      2097152.0        2        2      232 h
0.295352 0.292625      4194304      4194304.0       10       10      667 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.291697 h
total feature number = 973135350
Run:{3,None,0.5}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        4       34
0.750000 0.750000           16           16.0        7        9      106
0.812500 0.875000           32           32.0        1        4       25
0.859375 0.906250           64           64.0        3        9       25
0.828125 0.796875          128          128.0       10       10      589
0.785156 0.742188          256          256.0        5        5      367
0.695312 0.605469          512          512.0        4       10       22
0.637695 0.580078         1024         1024.0        7        1      142
0.569336 0.500977         2048         2048.0        7        7       19
0.500977 0.432617         4096         4096.0        3       10      157
0.451660 0.402344         8192         8192.0        7        8       40
0.414917 0.378174        16384        16384.0        6        6      136
0.384155 0.353394        32768        32768.0        2        2      838
0.361450 0.338745        65536        65536.0        4        4      151
0.342804 0.324158       131072       131072.0        2        2       85
0.327290 0.311775       262144       262144.0        6        7      100
0.315109 0.302929       524288       524288.0        2        2      196
0.305631 0.305631      1048576      1048576.0        7        7      286 h
0.300112 0.294592      2097152      2097152.0        2        2      232 h
0.297872 0.295633      4194304      4194304.0       10       10      667 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.294453 h
total feature number = 973135350
Run:{3,None,0.7}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1      571
1.000000 1.000000            2            2.0        4        9       25
0.500000 0.000000            4            4.0        9        9      268
0.750000 1.000000            8            8.0       10        4       34
0.750000 0.750000           16           16.0        7        9      106
0.843750 0.937500           32           32.0        1        4       25
0.875000 0.906250           64           64.0        3        9       25
0.828125 0.781250          128          128.0       10       10      589
0.785156 0.742188          256          256.0        5        5      367
0.693359 0.601562          512          512.0        4       10       22
0.632812 0.572266         1024         1024.0        7        1      142
0.566406 0.500000         2048         2048.0        7        7       19
0.500000 0.433594         4096         4096.0        3       10      157
0.449951 0.399902         8192         8192.0        7        8       40
0.412537 0.375122        16384        16384.0        6        6      136
0.382019 0.351501        32768        32768.0        2        2      838
0.359756 0.337494        65536        65536.0        4        4      151
0.341911 0.324066       131072       131072.0        2        2       85
0.327156 0.312401       262144       262144.0        6        7      100
0.315790 0.304424       524288       524288.0        2        2      196
0.307203 0.307203      1048576      1048576.0        7        7      286 h
0.302206 0.297209      2097152      2097152.0        2        2      232 h
0.300204 0.298202      4194304      4194304.0       10       10      667 h

finished run
number of examples per pass = 933334
passes used = 5
weighted example sum = 4666670.000000
weighted label sum = 0.000000
average loss = 0.297105 h
total feature number = 973135350
Run:{3,None,0.9}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1     1136
1.000000 1.000000            2            2.0        4        9       44
0.500000 0.000000            4            4.0        9        9      530
0.750000 1.000000            8            8.0       10        9       62
0.750000 0.750000           16           16.0        7        9      206
0.781250 0.812500           32           32.0        1        9       44
0.843750 0.906250           64           64.0        3        9       44
0.882812 0.921875          128          128.0       10        9     1172
0.851562 0.820312          256          256.0        5        9      728
0.828125 0.804688          512          512.0        4        9       38
0.814453 0.800781         1024         1024.0        7        9      278
0.764648 0.714844         2048         2048.0        7       10       32
0.714600 0.664551         4096         4096.0        3       10      308
0.681885 0.649170         8192         8192.0        7        9       74
0.669678 0.657471        16384        16384.0        6       10      266
0.653015 0.636353        32768        32768.0        2        3     1670
0.629181 0.605347        65536        65536.0        4       10      296
0.610329 0.591476       131072       131072.0        2       10      164
0.595554 0.580780       262144       262144.0        6       10      194
0.583950 0.572346       524288       524288.0        2       10      386
0.570566 0.570566      1048576      1048576.0        7        7      566 h
0.557993 0.545419      2097152      2097152.0        2        9      458 h
0.543619 0.529245      4194304      4194304.0       10       10     1328 h
0.526210 0.508801      8388608      8388608.0        8        3      242 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.478747 h
total feature number = 5754829785
Run:{3,1,0.001}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1     1136
1.000000 1.000000            2            2.0        4        9       44
0.500000 0.000000            4            4.0        9        9      530
0.750000 1.000000            8            8.0       10        9       62
0.750000 0.750000           16           16.0        7        9      206
0.781250 0.812500           32           32.0        1        9       44
0.843750 0.906250           64           64.0        3        9       44
0.875000 0.906250          128          128.0       10       10     1172
0.839844 0.804688          256          256.0        5        9      728
0.808594 0.777344          512          512.0        4       10       38
0.776367 0.744141         1024         1024.0        7        3      278
0.729004 0.681641         2048         2048.0        7       10       32
0.682129 0.635254         4096         4096.0        3       10      308
0.646973 0.611816         8192         8192.0        7        9       74
0.629211 0.611450        16384        16384.0        6       10      266
0.599396 0.569580        32768        32768.0        2        3     1670
0.566345 0.533295        65536        65536.0        4       10      296
0.538155 0.509964       131072       131072.0        2        2      164
0.509937 0.481720       262144       262144.0        6       10      194
0.477400 0.444862       524288       524288.0        2       10      386
0.443559 0.443559      1048576      1048576.0        7        7      566 h
0.414463 0.385367      2097152      2097152.0        2        4      458 h
0.390797 0.367130      4194304      4194304.0       10       10     1328 h
0.370943 0.351089      8388608      8388608.0        8        3      242 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.335452 h
total feature number = 5754829785
Run:{3,1,0.01}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1     1136
1.000000 1.000000            2            2.0        4        9       44
0.500000 0.000000            4            4.0        9        9      530
0.750000 1.000000            8            8.0       10        9       62
0.750000 0.750000           16           16.0        7        9      206
0.781250 0.812500           32           32.0        1        9       44
0.843750 0.906250           64           64.0        3        9       44
0.851562 0.859375          128          128.0       10       10     1172
0.812500 0.773438          256          256.0        5       10      728
0.750000 0.687500          512          512.0        4       10       38
0.704102 0.658203         1024         1024.0        7        3      278
0.645508 0.586914         2048         2048.0        7       10       32
0.593262 0.541016         4096         4096.0        3        1      308
0.540039 0.486816         8192         8192.0        7        5       74
0.491272 0.442505        16384        16384.0        6        8      266
0.448059 0.404846        32768        32768.0        2        3     1670
0.416794 0.385529        65536        65536.0        4        4      296
0.390221 0.363647       131072       131072.0        2        2      164
0.367218 0.344215       262144       262144.0        6       10      194
0.348368 0.329517       524288       524288.0        2        4      386
0.331951 0.331951      1048576      1048576.0        7        7      566 h
0.319904 0.307858      2097152      2097152.0        2        4      458 h
0.310781 0.301657      4194304      4194304.0       10       10     1328 h
0.304076 0.297371      8388608      8388608.0        8        8      242 h

finished run
number of examples per pass = 933334
passes used = 15
weighted example sum = 14000010.000000
weighted label sum = 0.000000
average loss = 0.295350 h
total feature number = 5754829785
Run:{3,1,0.1}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1     1136
1.000000 1.000000            2            2.0        4        9       44
0.500000 0.000000            4            4.0        9        9      530
0.750000 1.000000            8            8.0       10        9       62
0.750000 0.750000           16           16.0        7        9      206
0.781250 0.812500           32           32.0        1        4       44
0.843750 0.906250           64           64.0        3        9       44
0.828125 0.812500          128          128.0       10       10     1172
0.800781 0.773438          256          256.0        5       10      728
0.742188 0.683594          512          512.0        4       10       38
0.674805 0.607422         1024         1024.0        7        3      278
0.606445 0.538086         2048         2048.0        7        7       32
0.539795 0.473145         4096         4096.0        3       10      308
0.491211 0.442627         8192         8192.0        7        8       74
0.450623 0.410034        16384        16384.0        6        8      266
0.415039 0.379456        32768        32768.0        2        2     1670
0.389511 0.363983        65536        65536.0        4        4      296
0.366669 0.343826       131072       131072.0        2        2      164
0.346523 0.326378       262144       262144.0        6        6      194
0.330494 0.314465       524288       524288.0        2        4      386
0.316253 0.316253      1048576      1048576.0        7        7      566 h
0.307329 0.298405      2097152      2097152.0        2        2      458 h
0.302076 0.296822      4194304      4194304.0       10       10     1328 h

finished run
number of examples per pass = 933334
passes used = 8
weighted example sum = 7466672.000000
weighted label sum = 0.000000
average loss = 0.296405 h
total feature number = 3069242552
Run:{3,1,0.3}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1     1136
1.000000 1.000000            2            2.0        4        9       44
0.500000 0.000000            4            4.0        9        9      530
0.750000 1.000000            8            8.0       10        4       62
0.750000 0.750000           16           16.0        7        9      206
0.781250 0.812500           32           32.0        1        4       44
0.843750 0.906250           64           64.0        3        9       44
0.828125 0.812500          128          128.0       10       10     1172
0.785156 0.742188          256          256.0        5       10      728
0.734375 0.683594          512          512.0        4       10       38
0.665039 0.595703         1024         1024.0        7        3      278
0.599609 0.534180         2048         2048.0        7        7       32
0.529297 0.458984         4096         4096.0        3       10      308
0.481812 0.434326         8192         8192.0        7        8       74
0.442139 0.402466        16384        16384.0        6        8      266
0.407471 0.372803        32768        32768.0        2        2     1670
0.383377 0.359283        65536        65536.0        4        4      296
0.361687 0.339996       131072       131072.0        2        2      164
0.343166 0.324646       262144       262144.0        6        6      194
0.328217 0.313267       524288       524288.0        2        4      386
0.315246 0.315246      1048576      1048576.0        7        7      566 h
0.307270 0.299294      2097152      2097152.0        2        2      458 h
0.302913 0.298556      4194304      4194304.0       10       10     1328 h

finished run
number of examples per pass = 933334
passes used = 7
weighted example sum = 6533338.000000
weighted label sum = 0.000000
average loss = 0.298434 h
total feature number = 2685587233
Run:{3,1,0.5}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1     1136
1.000000 1.000000            2            2.0        4        9       44
0.500000 0.000000            4            4.0        9        9      530
0.750000 1.000000            8            8.0       10        4       62
0.750000 0.750000           16           16.0        7        9      206
0.781250 0.812500           32           32.0        1        4       44
0.843750 0.906250           64           64.0        3        9       44
0.828125 0.812500          128          128.0       10       10     1172
0.789062 0.750000          256          256.0        5       10      728
0.730469 0.671875          512          512.0        4       10       38
0.658203 0.585938         1024         1024.0        7        3      278
0.594727 0.531250         2048         2048.0        7        7       32
0.527588 0.460449         4096         4096.0        3       10      308
0.477905 0.428223         8192         8192.0        7        8       74
0.438354 0.398804        16384        16384.0        6        8      266
0.404358 0.370361        32768        32768.0        2        2     1670
0.380859 0.357361        65536        65536.0        4        4      296
0.359993 0.339127       131072       131072.0        2        2      164
0.342415 0.324837       262144       262144.0        6        6      194
0.328203 0.313992       524288       524288.0        2        4      386
0.316072 0.316072      1048576      1048576.0        7        7      566 h
0.308706 0.301341      2097152      2097152.0        2        2      458 h
0.304718 0.300730      4194304      4194304.0       10       10     1328 h

finished run
number of examples per pass = 933334
passes used = 6
weighted example sum = 5600004.000000
weighted label sum = 0.000000
average loss = 0.300365 h
total feature number = 2301931914
Run:{3,1,0.7}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/yahoo_answers_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        9        1     1136
1.000000 1.000000            2            2.0        4        9       44
0.500000 0.000000            4            4.0        9        9      530
0.625000 0.750000            8            8.0       10        4       62
0.687500 0.750000           16           16.0        7        9      206
0.781250 0.875000           32           32.0        1        4       44
0.843750 0.906250           64           64.0        3        9       44
0.828125 0.812500          128          128.0       10       10     1172
0.789062 0.750000          256          256.0        5       10      728
0.724609 0.660156          512          512.0        4       10       38
0.653320 0.582031         1024         1024.0        7        3      278
0.590332 0.527344         2048         2048.0        7        7       32
0.525879 0.461426         4096         4096.0        3       10      308
0.475830 0.425781         8192         8192.0        7        8       74
0.437500 0.399170        16384        16384.0        6        8      266
0.403229 0.368958        32768        32768.0        2        2     1670
0.379852 0.356476        65536        65536.0        4        4      296
0.359612 0.339371       131072       131072.0        2        2      164
0.342552 0.325493       262144       262144.0        6        6      194
0.328865 0.315178       524288       524288.0        2        4      386
0.317416 0.317416      1048576      1048576.0        7        7      566 h
0.310253 0.303090      2097152      2097152.0        2        2      458 h
0.306444 0.302636      4194304      4194304.0       10       10     1328 h

finished run
number of examples per pass = 933334
passes used = 6
weighted example sum = 5600004.000000
weighted label sum = 0.000000
average loss = 0.302390 h
total feature number = 2301931914
Run:{3,1,0.9}


