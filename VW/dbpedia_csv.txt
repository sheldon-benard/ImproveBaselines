Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.562500 0.375000           16           16.0        8       13       59
0.593750 0.625000           32           32.0        9        9       37
0.671875 0.750000           64           64.0       14       13       29
0.640625 0.609375          128          128.0        1        7       31
0.570312 0.500000          256          256.0        1       12       42
0.451172 0.332031          512          512.0        3       13       37
0.396484 0.341797         1024         1024.0       10       10       37
0.319824 0.243164         2048         2048.0        8        8       57
0.228027 0.136230         4096         4096.0        8        8       32
0.180420 0.132812         8192         8192.0        3        5       48
0.150818 0.121216        16384        16384.0        5        5       39
0.132446 0.114075        32768        32768.0       12       12       25
0.119705 0.106964        65536        65536.0       11       11       24
0.111336 0.102966       131072       131072.0       13       13       65
0.106533 0.101730       262144       262144.0        5        5       62
0.099870 0.099870       524288       524288.0        5        5       59 h
0.095500 0.091130      1048576      1048576.0        4        4       39 h
0.090304 0.085108      2097152      2097152.0        6        6       50 h
0.083935 0.077566      4194304      4194304.0        6        6       23 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.071556 h
total feature number = 219846660
Run:{1,None,0.001}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.562500 0.375000           16           16.0        8       13       59
0.593750 0.625000           32           32.0        9        9       37
0.671875 0.750000           64           64.0       14       13       29
0.640625 0.609375          128          128.0        1        7       31
0.562500 0.484375          256          256.0        1       12       42
0.443359 0.324219          512          512.0        3       13       37
0.374023 0.304688         1024         1024.0       10       10       37
0.293457 0.212891         2048         2048.0        8        8       57
0.206787 0.120117         4096         4096.0        8        8       32
0.161621 0.116455         8192         8192.0        3        5       48
0.133057 0.104492        16384        16384.0        5        5       39
0.112610 0.092163        32768        32768.0       12       12       25
0.097565 0.082520        65536        65536.0       11       11       24
0.085320 0.073074       131072       131072.0       13       13       65
0.076199 0.067078       262144       262144.0        5        5       62
0.065262 0.065262       524288       524288.0        5        5       59 h
0.057223 0.049183      1048576      1048576.0        4        4       39 h
0.050079 0.042935      2097152      2097152.0        6        6       50 h
0.044059 0.038039      4194304      4194304.0        6        6       23 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.034618 h
total feature number = 219846660
Run:{1,None,0.01}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.562500 0.375000           16           16.0        8       13       59
0.593750 0.625000           32           32.0        9        9       37
0.625000 0.656250           64           64.0       14       13       29
0.593750 0.562500          128          128.0        1        7       31
0.500000 0.406250          256          256.0        1       12       42
0.380859 0.261719          512          512.0        3        6       37
0.287109 0.193359         1024         1024.0       10       10       37
0.210449 0.133789         2048         2048.0        8        8       57
0.140869 0.071289         4096         4096.0        8        8       32
0.107788 0.074707         8192         8192.0        3        3       48
0.081787 0.055786        16384        16384.0        5        5       39
0.064301 0.046814        32768        32768.0       12       12       25
0.052002 0.039703        65536        65536.0       11       11       24
0.042862 0.033722       131072       131072.0       13       13       65
0.036892 0.030922       262144       262144.0        5        5       62
0.031254 0.031254       524288       524288.0        5        5       59 h
0.027559 0.023865      1048576      1048576.0        4        4       39 h
0.024640 0.021721      2097152      2097152.0        6        6       50 h
0.022428 0.020215      4194304      4194304.0        6        6       23 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.019366 h
total feature number = 219846660
Run:{1,None,0.1}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.562500 0.375000           16           16.0        8       13       59
0.593750 0.625000           32           32.0        9        9       37
0.609375 0.625000           64           64.0       14       13       29
0.546875 0.484375          128          128.0        1        7       31
0.453125 0.359375          256          256.0        1       12       42
0.326172 0.199219          512          512.0        3        6       37
0.224609 0.123047         1024         1024.0       10       10       37
0.161621 0.098633         2048         2048.0        8        8       57
0.107178 0.052734         4096         4096.0        8        8       32
0.082886 0.058594         8192         8192.0        3        3       48
0.062500 0.042114        16384        16384.0        5        5       39
0.049194 0.035889        32768        32768.0       12       12       25
0.039780 0.030365        65536        65536.0       11       11       24
0.032608 0.025436       131072       131072.0       13       13       65
0.028252 0.023895       262144       262144.0        5        5       62
0.024395 0.024395       524288       524288.0        5        5       59 h
0.021982 0.019569      1048576      1048576.0        4        4       39 h
0.020253 0.018524      2097152      2097152.0        6        6       50 h
0.019209 0.018164      4194304      4194304.0        6        6       23 h

finished run
number of examples per pass = 373334
passes used = 12
weighted example sum = 4480008.000000
weighted label sum = 0.000000
average loss = 0.018129 h
total feature number = 175877328
Run:{1,None,0.3}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.562500 0.375000           16           16.0        8       13       59
0.593750 0.625000           32           32.0        9        9       37
0.625000 0.656250           64           64.0       14       13       29
0.531250 0.437500          128          128.0        1        7       31
0.421875 0.312500          256          256.0        1       12       42
0.300781 0.179688          512          512.0        3        6       37
0.207031 0.113281         1024         1024.0       10       10       37
0.151855 0.096680         2048         2048.0        8        8       57
0.100830 0.049805         4096         4096.0        8        8       32
0.076782 0.052734         8192         8192.0        3        3       48
0.058350 0.039917        16384        16384.0        5        5       39
0.045959 0.033569        32768        32768.0       12       12       25
0.037186 0.028412        65536        65536.0       11       11       24
0.030350 0.023514       131072       131072.0       13       13       65
0.026299 0.022247       262144       262144.0        5        5       62
0.022789 0.022789       524288       524288.0        5        5       59 h
0.020739 0.018688      1048576      1048576.0        4        4       39 h
0.019427 0.018116      2097152      2097152.0        6        6       50 h

finished run
number of examples per pass = 373334
passes used = 8
weighted example sum = 2986672.000000
weighted label sum = 0.000000
average loss = 0.018113 h
total feature number = 117251552
Run:{1,None,0.5}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.500000 0.250000           16           16.0        8        8       59
0.562500 0.625000           32           32.0        9        9       37
0.609375 0.656250           64           64.0       14       13       29
0.515625 0.421875          128          128.0        1        6       31
0.398438 0.281250          256          256.0        1       12       42
0.283203 0.167969          512          512.0        3        5       37
0.197266 0.111328         1024         1024.0       10       10       37
0.146973 0.096680         2048         2048.0        8        8       57
0.099121 0.051270         4096         4096.0        8        8       32
0.075684 0.052246         8192         8192.0        3        3       48
0.056763 0.037842        16384        16384.0        5        5       39
0.044434 0.032104        32768        32768.0       12       12       25
0.036118 0.027802        65536        65536.0       11       11       24
0.029327 0.022537       131072       131072.0       13       13       65
0.025604 0.021881       262144       262144.0        5        5       62
0.022240 0.022240       524288       524288.0        5        5       59 h
0.020355 0.018471      1048576      1048576.0        4        4       39 h
0.019292 0.018229      2097152      2097152.0        6        6       50 h

finished run
number of examples per pass = 373334
passes used = 7
weighted example sum = 2613338.000000
weighted label sum = 0.000000
average loss = 0.018171 h
total feature number = 102595108
Run:{1,None,0.7}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.500000 0.250000           16           16.0        8        8       59
0.562500 0.625000           32           32.0        9        9       37
0.593750 0.625000           64           64.0       14       13       29
0.492188 0.390625          128          128.0        1        6       31
0.386719 0.281250          256          256.0        1       12       42
0.275391 0.164062          512          512.0        3        5       37
0.192383 0.109375         1024         1024.0       10       10       37
0.143555 0.094727         2048         2048.0        8        8       57
0.097168 0.050781         4096         4096.0        8        8       32
0.074585 0.052002         8192         8192.0        3        3       48
0.055420 0.036255        16384        16384.0        5        5       39
0.043518 0.031616        32768        32768.0       12       12       25
0.035660 0.027802        65536        65536.0       11       11       24
0.028915 0.022171       131072       131072.0       13       13       65
0.025230 0.021545       262144       262144.0        5        5       62
0.021843 0.021843       524288       524288.0        5        5       59 h
0.020203 0.018562      1048576      1048576.0        4        4       39 h
0.019392 0.018581      2097152      2097152.0        6        6       50 h

finished run
number of examples per pass = 373334
passes used = 6
weighted example sum = 2240004.000000
weighted label sum = 0.000000
average loss = 0.018354 h
total feature number = 87938664
Run:{1,None,0.9}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.562500 0.375000           16           16.0        8       13       59
0.593750 0.625000           32           32.0        9        9       37
0.671875 0.750000           64           64.0       14       13       29
0.640625 0.609375          128          128.0        1        7       31
0.570312 0.500000          256          256.0        1       12       42
0.451172 0.332031          512          512.0        3       13       37
0.396484 0.341797         1024         1024.0       10       10       37
0.319824 0.243164         2048         2048.0        8        8       57
0.228027 0.136230         4096         4096.0        8        8       32
0.180420 0.132812         8192         8192.0        3        5       48
0.150818 0.121216        16384        16384.0        5        5       39
0.132446 0.114075        32768        32768.0       12       12       25
0.119705 0.106964        65536        65536.0       11       11       24
0.111336 0.102966       131072       131072.0       13       13       65
0.106533 0.101730       262144       262144.0        5        5       62
0.099870 0.099870       524288       524288.0        5        5       59 h
0.095500 0.091130      1048576      1048576.0        4        4       39 h
0.090304 0.085108      2097152      2097152.0        6        6       50 h
0.083935 0.077566      4194304      4194304.0        6        6       23 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.071556 h
total feature number = 219846660
Run:{1,1,0.001}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.562500 0.375000           16           16.0        8       13       59
0.593750 0.625000           32           32.0        9        9       37
0.671875 0.750000           64           64.0       14       13       29
0.640625 0.609375          128          128.0        1        7       31
0.562500 0.484375          256          256.0        1       12       42
0.443359 0.324219          512          512.0        3       13       37
0.374023 0.304688         1024         1024.0       10       10       37
0.293457 0.212891         2048         2048.0        8        8       57
0.206787 0.120117         4096         4096.0        8        8       32
0.161621 0.116455         8192         8192.0        3        5       48
0.133057 0.104492        16384        16384.0        5        5       39
0.112610 0.092163        32768        32768.0       12       12       25
0.097565 0.082520        65536        65536.0       11       11       24
0.085320 0.073074       131072       131072.0       13       13       65
0.076199 0.067078       262144       262144.0        5        5       62
0.065262 0.065262       524288       524288.0        5        5       59 h
0.057223 0.049183      1048576      1048576.0        4        4       39 h
0.050079 0.042935      2097152      2097152.0        6        6       50 h
0.044059 0.038039      4194304      4194304.0        6        6       23 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.034618 h
total feature number = 219846660
Run:{1,1,0.01}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.562500 0.375000           16           16.0        8       13       59
0.593750 0.625000           32           32.0        9        9       37
0.625000 0.656250           64           64.0       14       13       29
0.593750 0.562500          128          128.0        1        7       31
0.500000 0.406250          256          256.0        1       12       42
0.380859 0.261719          512          512.0        3        6       37
0.287109 0.193359         1024         1024.0       10       10       37
0.210449 0.133789         2048         2048.0        8        8       57
0.140869 0.071289         4096         4096.0        8        8       32
0.107788 0.074707         8192         8192.0        3        3       48
0.081787 0.055786        16384        16384.0        5        5       39
0.064301 0.046814        32768        32768.0       12       12       25
0.052002 0.039703        65536        65536.0       11       11       24
0.042862 0.033722       131072       131072.0       13       13       65
0.036892 0.030922       262144       262144.0        5        5       62
0.031254 0.031254       524288       524288.0        5        5       59 h
0.027559 0.023865      1048576      1048576.0        4        4       39 h
0.024640 0.021721      2097152      2097152.0        6        6       50 h
0.022428 0.020215      4194304      4194304.0        6        6       23 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.019366 h
total feature number = 219846660
Run:{1,1,0.1}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.562500 0.375000           16           16.0        8       13       59
0.593750 0.625000           32           32.0        9        9       37
0.609375 0.625000           64           64.0       14       13       29
0.546875 0.484375          128          128.0        1        7       31
0.453125 0.359375          256          256.0        1       12       42
0.326172 0.199219          512          512.0        3        6       37
0.224609 0.123047         1024         1024.0       10       10       37
0.161621 0.098633         2048         2048.0        8        8       57
0.107178 0.052734         4096         4096.0        8        8       32
0.082886 0.058594         8192         8192.0        3        3       48
0.062500 0.042114        16384        16384.0        5        5       39
0.049194 0.035889        32768        32768.0       12       12       25
0.039780 0.030365        65536        65536.0       11       11       24
0.032608 0.025436       131072       131072.0       13       13       65
0.028252 0.023895       262144       262144.0        5        5       62
0.024395 0.024395       524288       524288.0        5        5       59 h
0.021982 0.019569      1048576      1048576.0        4        4       39 h
0.020253 0.018524      2097152      2097152.0        6        6       50 h
0.019209 0.018164      4194304      4194304.0        6        6       23 h

finished run
number of examples per pass = 373334
passes used = 12
weighted example sum = 4480008.000000
weighted label sum = 0.000000
average loss = 0.018129 h
total feature number = 175877328
Run:{1,1,0.3}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.562500 0.375000           16           16.0        8       13       59
0.593750 0.625000           32           32.0        9        9       37
0.625000 0.656250           64           64.0       14       13       29
0.531250 0.437500          128          128.0        1        7       31
0.421875 0.312500          256          256.0        1       12       42
0.300781 0.179688          512          512.0        3        6       37
0.207031 0.113281         1024         1024.0       10       10       37
0.151855 0.096680         2048         2048.0        8        8       57
0.100830 0.049805         4096         4096.0        8        8       32
0.076782 0.052734         8192         8192.0        3        3       48
0.058350 0.039917        16384        16384.0        5        5       39
0.045959 0.033569        32768        32768.0       12       12       25
0.037186 0.028412        65536        65536.0       11       11       24
0.030350 0.023514       131072       131072.0       13       13       65
0.026299 0.022247       262144       262144.0        5        5       62
0.022789 0.022789       524288       524288.0        5        5       59 h
0.020739 0.018688      1048576      1048576.0        4        4       39 h
0.019427 0.018116      2097152      2097152.0        6        6       50 h

finished run
number of examples per pass = 373334
passes used = 8
weighted example sum = 2986672.000000
weighted label sum = 0.000000
average loss = 0.018113 h
total feature number = 117251552
Run:{1,1,0.5}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.500000 0.250000           16           16.0        8        8       59
0.562500 0.625000           32           32.0        9        9       37
0.609375 0.656250           64           64.0       14       13       29
0.515625 0.421875          128          128.0        1        6       31
0.398438 0.281250          256          256.0        1       12       42
0.283203 0.167969          512          512.0        3        5       37
0.197266 0.111328         1024         1024.0       10       10       37
0.146973 0.096680         2048         2048.0        8        8       57
0.099121 0.051270         4096         4096.0        8        8       32
0.075684 0.052246         8192         8192.0        3        3       48
0.056763 0.037842        16384        16384.0        5        5       39
0.044434 0.032104        32768        32768.0       12       12       25
0.036118 0.027802        65536        65536.0       11       11       24
0.029327 0.022537       131072       131072.0       13       13       65
0.025604 0.021881       262144       262144.0        5        5       62
0.022240 0.022240       524288       524288.0        5        5       59 h
0.020355 0.018471      1048576      1048576.0        4        4       39 h
0.019292 0.018229      2097152      2097152.0        6        6       50 h

finished run
number of examples per pass = 373334
passes used = 7
weighted example sum = 2613338.000000
weighted label sum = 0.000000
average loss = 0.018171 h
total feature number = 102595108
Run:{1,1,0.7}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1       58
1.000000 1.000000            2            2.0        9        3       16
0.750000 0.500000            4            4.0        9        9       45
0.750000 0.750000            8            8.0        8        8        6
0.500000 0.250000           16           16.0        8        8       59
0.562500 0.625000           32           32.0        9        9       37
0.593750 0.625000           64           64.0       14       13       29
0.492188 0.390625          128          128.0        1        6       31
0.386719 0.281250          256          256.0        1       12       42
0.275391 0.164062          512          512.0        3        5       37
0.192383 0.109375         1024         1024.0       10       10       37
0.143555 0.094727         2048         2048.0        8        8       57
0.097168 0.050781         4096         4096.0        8        8       32
0.074585 0.052002         8192         8192.0        3        3       48
0.055420 0.036255        16384        16384.0        5        5       39
0.043518 0.031616        32768        32768.0       12       12       25
0.035660 0.027802        65536        65536.0       11       11       24
0.028915 0.022171       131072       131072.0       13       13       65
0.025230 0.021545       262144       262144.0        5        5       62
0.021843 0.021843       524288       524288.0        5        5       59 h
0.020203 0.018562      1048576      1048576.0        4        4       39 h
0.019392 0.018581      2097152      2097152.0        6        6       50 h

finished run
number of examples per pass = 373334
passes used = 6
weighted example sum = 2240004.000000
weighted label sum = 0.000000
average loss = 0.018354 h
total feature number = 87938664
Run:{1,1,0.9}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      114
1.000000 1.000000            2            2.0        9        3       30
0.750000 0.500000            4            4.0        9        9       88
0.750000 0.750000            8            8.0        8        8       10
0.562500 0.375000           16           16.0        8       13      116
0.593750 0.625000           32           32.0        9        9       72
0.640625 0.687500           64           64.0       14       13       56
0.617188 0.593750          128          128.0        1        7       60
0.546875 0.476562          256          256.0        1       12       82
0.423828 0.300781          512          512.0        3       13       72
0.350586 0.277344         1024         1024.0       10       10       72
0.275879 0.201172         2048         2048.0        8        8      112
0.195312 0.114746         4096         4096.0        8        8       62
0.151978 0.108643         8192         8192.0        3        5       94
0.127197 0.102417        16384        16384.0        5        5       76
0.112000 0.096802        32768        32768.0       12       12       48
0.100998 0.089996        65536        65536.0       11       11       46
0.093697 0.086395       131072       131072.0       13       13      128
0.090195 0.086693       262144       262144.0        5        5      122
0.084515 0.084515       524288       524288.0        5        5      116 h
0.080994 0.077473      1048576      1048576.0        4        4       76 h
0.077199 0.073405      2097152      2097152.0        6        6       98 h
0.072733 0.068267      4194304      4194304.0        6        6       44 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.063552 h
total feature number = 428493300
Run:{2,None,0.001}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      114
1.000000 1.000000            2            2.0        9        3       30
0.750000 0.500000            4            4.0        9        9       88
0.750000 0.750000            8            8.0        8        8       10
0.562500 0.375000           16           16.0        8       13      116
0.593750 0.625000           32           32.0        9        9       72
0.640625 0.687500           64           64.0       14       13       56
0.609375 0.578125          128          128.0        1        7       60
0.531250 0.453125          256          256.0        1       12       82
0.410156 0.289062          512          512.0        3       13       72
0.333984 0.257812         1024         1024.0       10       10       72
0.259766 0.185547         2048         2048.0        8        8      112
0.182129 0.104492         4096         4096.0        8        8       62
0.142090 0.102051         8192         8192.0        3        5       94
0.116455 0.090820        16384        16384.0        5        5       76
0.099121 0.081787        32768        32768.0       12       12       48
0.086502 0.073883        65536        65536.0       11       11       46
0.076134 0.065765       131072       131072.0       13       13      128
0.068684 0.061234       262144       262144.0        5        5      122
0.059353 0.059353       524288       524288.0        5        5      116 h
0.052279 0.045204      1048576      1048576.0        4        4       76 h
0.045689 0.039099      2097152      2097152.0        6        6       98 h
0.040098 0.034507      4194304      4194304.0        6        6       44 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.031211 h
total feature number = 428493300
Run:{2,None,0.01}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      114
1.000000 1.000000            2            2.0        9        3       30
0.750000 0.500000            4            4.0        9        9       88
0.750000 0.750000            8            8.0        8        8       10
0.562500 0.375000           16           16.0        8       13      116
0.593750 0.625000           32           32.0        9        9       72
0.609375 0.625000           64           64.0       14       13       56
0.562500 0.515625          128          128.0        1        7       60
0.476562 0.390625          256          256.0        1       12       82
0.361328 0.246094          512          512.0        3       13       72
0.269531 0.177734         1024         1024.0       10       10       72
0.201172 0.132812         2048         2048.0        8        8      112
0.135742 0.070312         4096         4096.0        8        8       62
0.102173 0.068604         8192         8192.0        3        3       94
0.078003 0.053833        16384        16384.0        5        5       76
0.061035 0.044067        32768        32768.0       12       12       48
0.049698 0.038361        65536        65536.0       11       11       46
0.040276 0.030853       131072       131072.0       13       13      128
0.034206 0.028137       262144       262144.0        5        5      122
0.028813 0.028813       524288       524288.0        5        5      116 h
0.024977 0.021141      1048576      1048576.0        4        4       76 h
0.022132 0.019287      2097152      2097152.0        6        6       98 h
0.020085 0.018039      4194304      4194304.0        6        6       44 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.017046 h
total feature number = 428493300
Run:{2,None,0.1}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      114
1.000000 1.000000            2            2.0        9        3       30
0.750000 0.500000            4            4.0        9        9       88
0.750000 0.750000            8            8.0        8        8       10
0.562500 0.375000           16           16.0        8       13      116
0.593750 0.625000           32           32.0        9        9       72
0.593750 0.593750           64           64.0       14       13       56
0.546875 0.500000          128          128.0        1        7       60
0.449219 0.351562          256          256.0        1       12       82
0.330078 0.210938          512          512.0        3        6       72
0.228516 0.126953         1024         1024.0       10       10       72
0.166504 0.104492         2048         2048.0        8        8      112
0.109375 0.052246         4096         4096.0        8        8       62
0.083252 0.057129         8192         8192.0        3        3       94
0.062195 0.041138        16384        16384.0        5        5       76
0.048462 0.034729        32768        32768.0       12       12       48
0.038895 0.029327        65536        65536.0       11       11       46
0.031044 0.023193       131072       131072.0       13       13      128
0.026249 0.021454       262144       262144.0        5        5      122
0.022301 0.022301       524288       524288.0        5        5      116 h
0.019781 0.017262      1048576      1048576.0        4        4       76 h
0.018011 0.016241      2097152      2097152.0        6        6       98 h
0.016903 0.015794      4194304      4194304.0        6        6       44 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.015595 h
total feature number = 428493300
Run:{2,None,0.3}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      114
1.000000 1.000000            2            2.0        9        3       30
0.750000 0.500000            4            4.0        9        9       88
0.750000 0.750000            8            8.0        8        8       10
0.562500 0.375000           16           16.0        8       13      116
0.593750 0.625000           32           32.0        9        9       72
0.593750 0.593750           64           64.0       14       13       56
0.531250 0.468750          128          128.0        1        7       60
0.425781 0.320312          256          256.0        1       12       82
0.308594 0.191406          512          512.0        3        6       72
0.210938 0.113281         1024         1024.0       10       10       72
0.155762 0.100586         2048         2048.0        8        8      112
0.103516 0.051270         4096         4096.0        8        8       62
0.078857 0.054199         8192         8192.0        3        3       94
0.059082 0.039307        16384        16384.0        5        5       76
0.045837 0.032593        32768        32768.0       12       12       48
0.036377 0.026917        65536        65536.0       11       11       46
0.028900 0.021423       131072       131072.0       13       13      128
0.024452 0.020004       262144       262144.0        5        5      122
0.020649 0.020649       524288       524288.0        5        5      116 h
0.018511 0.016373      1048576      1048576.0        4        4       76 h
0.017138 0.015764      2097152      2097152.0        6        6       98 h
0.016394 0.015650      4194304      4194304.0        6        6       44 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.015429 h
total feature number = 428493300
Run:{2,None,0.5}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      114
1.000000 1.000000            2            2.0        9        3       30
0.750000 0.500000            4            4.0        9        9       88
0.750000 0.750000            8            8.0        8        8       10
0.500000 0.250000           16           16.0        8        8      116
0.562500 0.625000           32           32.0        9        9       72
0.578125 0.593750           64           64.0       14       13       56
0.523438 0.468750          128          128.0        1        7       60
0.414062 0.304688          256          256.0        1       12       82
0.300781 0.187500          512          512.0        3        6       72
0.205078 0.109375         1024         1024.0       10       10       72
0.151855 0.098633         2048         2048.0        8        8      112
0.101562 0.051270         4096         4096.0        8        8       62
0.076782 0.052002         8192         8192.0        3        3       94
0.057495 0.038208        16384        16384.0        5        5       76
0.044464 0.031433        32768        32768.0       12       12       48
0.035202 0.025940        65536        65536.0       11       11       46
0.028023 0.020844       131072       131072.0       13       13      128
0.023624 0.019226       262144       262144.0        5        5      122
0.019982 0.019982       524288       524288.0        5        5      116 h
0.018030 0.016079      1048576      1048576.0        4        4       76 h
0.016793 0.015556      2097152      2097152.0        6        6       98 h
0.016131 0.015470      4194304      4194304.0        6        6       44 h

finished run
number of examples per pass = 373334
passes used = 12
weighted example sum = 4480008.000000
weighted label sum = 0.000000
average loss = 0.015418 h
total feature number = 342794640
Run:{2,None,0.7}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      114
1.000000 1.000000            2            2.0        9        3       30
0.750000 0.500000            4            4.0        9        9       88
0.750000 0.750000            8            8.0        8        8       10
0.500000 0.250000           16           16.0        8        8      116
0.562500 0.625000           32           32.0        9        9       72
0.562500 0.562500           64           64.0       14       13       56
0.507812 0.453125          128          128.0        1        7       60
0.398438 0.289062          256          256.0        1       12       82
0.285156 0.171875          512          512.0        3        5       72
0.197266 0.109375         1024         1024.0       10       10       72
0.146973 0.096680         2048         2048.0        8        8      112
0.098877 0.050781         4096         4096.0        8        8       62
0.075073 0.051270         8192         8192.0        3        3       94
0.056152 0.037231        16384        16384.0        5        5       76
0.043457 0.030762        32768        32768.0       12       12       48
0.034470 0.025482        65536        65536.0       11       11       46
0.027435 0.020401       131072       131072.0       13       13      128
0.023132 0.018829       262144       262144.0        5        5      122
0.019684 0.019684       524288       524288.0        5        5      116 h
0.017799 0.015915      1048576      1048576.0        4        4       76 h
0.016617 0.015434      2097152      2097152.0        6        6       98 h

finished run
number of examples per pass = 373334
passes used = 9
weighted example sum = 3360006.000000
weighted label sum = 0.000000
average loss = 0.015450 h
total feature number = 257095980
Run:{2,None,0.9}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.562500 0.375000           16           16.0        8       13      172
0.593750 0.625000           32           32.0        9        9      106
0.625000 0.656250           64           64.0       14       13       82
0.601562 0.578125          128          128.0        1        7       88
0.523438 0.445312          256          256.0        1       12      121
0.406250 0.289062          512          512.0        3        2      106
0.326172 0.246094         1024         1024.0       10       10      106
0.257324 0.188477         2048         2048.0        8        8      166
0.181396 0.105469         4096         4096.0        8        8       91
0.143677 0.105957         8192         8192.0        3        5      139
0.118835 0.093994        16384        16384.0        5        5      112
0.104340 0.089844        32768        32768.0       12       12       70
0.094177 0.084015        65536        65536.0       11       11       67
0.087265 0.080353       131072       131072.0       13       13      190
0.084209 0.081154       262144       262144.0        5        5      181
0.079091 0.079091       524288       524288.0        5        5      172 h
0.075884 0.072678      1048576      1048576.0        4        4      112 h
0.072520 0.069155      2097152      2097152.0        6        6      145 h
0.068982 0.065443      4194304      4194304.0        6        6       64 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.061398 h
total feature number = 631539945
Run:{2,1,0.001}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.562500 0.375000           16           16.0        8       13      172
0.593750 0.625000           32           32.0        9        9      106
0.625000 0.656250           64           64.0       14       13       82
0.601562 0.578125          128          128.0        1        7       88
0.523438 0.445312          256          256.0        1       12      121
0.402344 0.281250          512          512.0        3        2      106
0.320312 0.238281         1024         1024.0       10       10      106
0.250000 0.179688         2048         2048.0        8        8      166
0.175049 0.100098         4096         4096.0        8        8       91
0.136963 0.098877         8192         8192.0        3        5      139
0.112000 0.087036        16384        16384.0        5        5      112
0.093994 0.075989        32768        32768.0       12       12       70
0.082062 0.070129        65536        65536.0       11       11       67
0.072136 0.062210       131072       131072.0       13       13      190
0.065636 0.059135       262144       262144.0        5        5      181
0.057560 0.057560       524288       524288.0        5        5      172 h
0.050892 0.044224      1048576      1048576.0        4        4      112 h
0.044630 0.038368      2097152      2097152.0        6        6      145 h
0.039203 0.033776      4194304      4194304.0        6        6       64 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.030316 h
total feature number = 631539945
Run:{2,1,0.01}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.562500 0.375000           16           16.0        8       13      172
0.593750 0.625000           32           32.0        9        9      106
0.593750 0.593750           64           64.0       14       13       82
0.554688 0.515625          128          128.0        1        7       88
0.476562 0.398438          256          256.0        1       12      121
0.357422 0.238281          512          512.0        3        2      106
0.265625 0.173828         1024         1024.0       10       10      106
0.197266 0.128906         2048         2048.0        8        8      166
0.132324 0.067383         4096         4096.0        8        8       91
0.101685 0.071045         8192         8192.0        3        5      139
0.079041 0.056396        16384        16384.0        5        5      112
0.061554 0.044067        32768        32768.0       12       12       70
0.049744 0.037933        65536        65536.0       11       11       67
0.040527 0.031311       131072       131072.0       13       13      190
0.034061 0.027596       262144       262144.0        5        5      181
0.028755 0.028755       524288       524288.0        5        5      172 h
0.024849 0.020943      1048576      1048576.0        4        4      112 h
0.021981 0.019114      2097152      2097152.0        6        6      145 h
0.019874 0.017767      4194304      4194304.0        6        6       64 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.017089 h
total feature number = 631539945
Run:{2,1,0.1}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.562500 0.375000           16           16.0        8       13      172
0.593750 0.625000           32           32.0        9        9      106
0.593750 0.593750           64           64.0       14       13       82
0.531250 0.468750          128          128.0        1        7       88
0.437500 0.343750          256          256.0        1       12      121
0.326172 0.214844          512          512.0        3        2      106
0.224609 0.123047         1024         1024.0       10       10      106
0.168457 0.112305         2048         2048.0        8        8      166
0.111816 0.055176         4096         4096.0        8        8       91
0.084351 0.056885         8192         8192.0        3        3      139
0.062988 0.041626        16384        16384.0        5        5      112
0.048798 0.034607        32768        32768.0       12       12       70
0.039169 0.029541        65536        65536.0       11       11       67
0.031548 0.023926       131072       131072.0       13       13      190
0.026562 0.021576       262144       262144.0        5        5      181
0.022316 0.022316       524288       524288.0        5        5      172 h
0.019743 0.017170      1048576      1048576.0        4        4      112 h
0.017908 0.016073      2097152      2097152.0        6        6      145 h
0.016741 0.015574      4194304      4194304.0        6        6       64 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.015295 h
total feature number = 631539945
Run:{2,1,0.3}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.500000 0.250000           16           16.0        8        8      172
0.562500 0.625000           32           32.0        9        9      106
0.562500 0.562500           64           64.0       14       13       82
0.515625 0.468750          128          128.0        1        7       88
0.417969 0.320312          256          256.0        1       12      121
0.304688 0.191406          512          512.0        3        2      106
0.212891 0.121094         1024         1024.0       10       10      106
0.159668 0.106445         2048         2048.0        8        8      166
0.106445 0.053223         4096         4096.0        8        8       91
0.079956 0.053467         8192         8192.0        3        3      139
0.059387 0.038818        16384        16384.0        5        5      112
0.045929 0.032471        32768        32768.0       12       12       70
0.036713 0.027496        65536        65536.0       11       11       67
0.029312 0.021912       131072       131072.0       13       13      190
0.024551 0.019791       262144       262144.0        5        5      181
0.020634 0.020634       524288       524288.0        5        5      172 h
0.018372 0.016110      1048576      1048576.0        4        4      112 h
0.016884 0.015396      2097152      2097152.0        6        6      145 h
0.016033 0.015182      4194304      4194304.0        6        6       64 h

finished run
number of examples per pass = 373334
passes used = 14
weighted example sum = 5226676.000000
weighted label sum = 0.000000
average loss = 0.015107 h
total feature number = 589437282
Run:{2,1,0.5}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.500000 0.250000           16           16.0        8        8      172
0.562500 0.625000           32           32.0        9        9      106
0.562500 0.562500           64           64.0       14       13       82
0.500000 0.437500          128          128.0        1        7       88
0.406250 0.312500          256          256.0        1       12      121
0.296875 0.187500          512          512.0        3        2      106
0.205078 0.113281         1024         1024.0       10       10      106
0.153320 0.101562         2048         2048.0        8        8      166
0.103271 0.053223         4096         4096.0        8        8       91
0.078125 0.052979         8192         8192.0        3        3      139
0.058228 0.038330        16384        16384.0        5        5      112
0.044617 0.031006        32768        32768.0       12       12       70
0.035416 0.026215        65536        65536.0       11       11       67
0.028191 0.020966       131072       131072.0       13       13      190
0.023640 0.019089       262144       262144.0        5        5      181
0.019966 0.019966       524288       524288.0        5        5      172 h
0.017861 0.015755      1048576      1048576.0        4        4      112 h
0.016513 0.015165      2097152      2097152.0        6        6      145 h

finished run
number of examples per pass = 373334
passes used = 11
weighted example sum = 4106674.000000
weighted label sum = 0.000000
average loss = 0.015096 h
total feature number = 463129293
Run:{2,1,0.7}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.500000 0.250000           16           16.0        8        8      172
0.562500 0.625000           32           32.0        9        9      106
0.562500 0.562500           64           64.0       14       13       82
0.500000 0.437500          128          128.0        1        7       88
0.410156 0.320312          256          256.0        1       12      121
0.298828 0.187500          512          512.0        3        2      106
0.206055 0.113281         1024         1024.0       10       10      106
0.153809 0.101562         2048         2048.0        8        8      166
0.103271 0.052734         4096         4096.0        8        8       91
0.077881 0.052490         8192         8192.0        3        3      139
0.057983 0.038086        16384        16384.0        5        5      112
0.044403 0.030823        32768        32768.0       12       12       70
0.035049 0.025696        65536        65536.0       11       11       67
0.027870 0.020691       131072       131072.0       13       13      190
0.023289 0.018707       262144       262144.0        5        5      181
0.019604 0.019604       524288       524288.0        5        5      172 h
0.017601 0.015598      1048576      1048576.0        4        4      112 h
0.016359 0.015118      2097152      2097152.0        6        6      145 h

finished run
number of examples per pass = 373334
passes used = 8
weighted example sum = 2986672.000000
weighted label sum = 0.000000
average loss = 0.015129 h
total feature number = 336821304
Run:{2,1,0.9}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.562500 0.375000           16           16.0        8       13      172
0.593750 0.625000           32           32.0        9        9      106
0.640625 0.687500           64           64.0       14       13       82
0.617188 0.593750          128          128.0        1        7       88
0.539062 0.460938          256          256.0        1       12      121
0.419922 0.300781          512          512.0        3       13      106
0.345703 0.271484         1024         1024.0       10       10      106
0.272461 0.199219         2048         2048.0        8        8      166
0.193115 0.113770         4096         4096.0        8        8       91
0.149414 0.105713         8192         8192.0        3        5      139
0.125061 0.100708        16384        16384.0        5        5      112
0.109802 0.094543        32768        32768.0       12       12       70
0.098572 0.087341        65536        65536.0       11       11       67
0.091209 0.083847       131072       131072.0       13       13      190
0.087593 0.083977       262144       262144.0        5        5      181
0.082062 0.082062       524288       524288.0        5        5      172 h
0.078722 0.075383      1048576      1048576.0        4        4      112 h
0.075202 0.071682      2097152      2097152.0        6        6      145 h
0.071339 0.067476      4194304      4194304.0        6        6       64 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.063059 h
total feature number = 631539945
Run:{3,None,0.001}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.562500 0.375000           16           16.0        8       13      172
0.593750 0.625000           32           32.0        9        9      106
0.640625 0.687500           64           64.0       14       13       82
0.617188 0.593750          128          128.0        1        7       88
0.535156 0.453125          256          256.0        1       12      121
0.416016 0.296875          512          512.0        3       13      106
0.337891 0.259766         1024         1024.0       10       10      106
0.263184 0.188477         2048         2048.0        8        8      166
0.183594 0.104004         4096         4096.0        8        8       91
0.140991 0.098389         8192         8192.0        3        5      139
0.115784 0.090576        16384        16384.0        5        5      112
0.097961 0.080139        32768        32768.0       12       12       70
0.085480 0.072998        65536        65536.0       11       11       67
0.075493 0.065506       131072       131072.0       13       13      190
0.068768 0.062042       262144       262144.0        5        5      181
0.059891 0.059891       524288       524288.0        5        5      172 h
0.052958 0.046024      1048576      1048576.0        4        4      112 h
0.046542 0.040127      2097152      2097152.0        6        6      145 h
0.040824 0.035105      4194304      4194304.0        6        6       64 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.031666 h
total feature number = 631539945
Run:{3,None,0.01}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.562500 0.375000           16           16.0        8       13      172
0.593750 0.625000           32           32.0        9        9      106
0.609375 0.625000           64           64.0       14       13       82
0.570312 0.531250          128          128.0        1        7       88
0.480469 0.390625          256          256.0        1       12      121
0.369141 0.257812          512          512.0        3       13      106
0.276367 0.183594         1024         1024.0       10       10      106
0.207031 0.137695         2048         2048.0        8        8      166
0.138916 0.070801         4096         4096.0        8        8       91
0.104004 0.069092         8192         8192.0        3        5      139
0.080933 0.057861        16384        16384.0        5        5      112
0.063782 0.046631        32768        32768.0       12       12       70
0.051834 0.039886        65536        65536.0       11       11       67
0.041939 0.032043       131072       131072.0       13       13      190
0.035507 0.029076       262144       262144.0        5        5      181
0.029827 0.029827       524288       524288.0        5        5      172 h
0.025910 0.021992      1048576      1048576.0        4        4      112 h
0.022958 0.020006      2097152      2097152.0        6        6      145 h
0.020796 0.018635      4194304      4194304.0        6        6       64 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.017866 h
total feature number = 631539945
Run:{3,None,0.1}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.562500 0.375000           16           16.0        8       13      172
0.593750 0.625000           32           32.0        9        9      106
0.593750 0.593750           64           64.0       14       13       82
0.539062 0.484375          128          128.0        1        7       88
0.445312 0.351562          256          256.0        1       12      121
0.332031 0.218750          512          512.0        3        6      106
0.235352 0.138672         1024         1024.0       10       10      106
0.174316 0.113281         2048         2048.0        8        8      166
0.114258 0.054199         4096         4096.0        8        8       91
0.086914 0.059570         8192         8192.0        3        3      139
0.065063 0.043213        16384        16384.0        5        5      112
0.050720 0.036377        32768        32768.0       12       12       70
0.040894 0.031067        65536        65536.0       11       11       67
0.032784 0.024673       131072       131072.0       13       13      190
0.027695 0.022606       262144       262144.0        5        5      181
0.023350 0.023350       524288       524288.0        5        5      172 h
0.020649 0.017948      1048576      1048576.0        4        4      112 h
0.018792 0.016935      2097152      2097152.0        6        6      145 h
0.017679 0.016565      4194304      4194304.0        6        6       64 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.016371 h
total feature number = 631539945
Run:{3,None,0.3}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.562500 0.375000           16           16.0        8       13      172
0.593750 0.625000           32           32.0        9        9      106
0.593750 0.593750           64           64.0       14       13       82
0.531250 0.468750          128          128.0        1        7       88
0.437500 0.343750          256          256.0        1       12      121
0.324219 0.210938          512          512.0        3        6      106
0.222656 0.121094         1024         1024.0       10       10      106
0.164551 0.106445         2048         2048.0        8        8      166
0.107422 0.050293         4096         4096.0        8        8       91
0.080688 0.053955         8192         8192.0        3        3      139
0.060486 0.040283        16384        16384.0        5        5      112
0.047333 0.034180        32768        32768.0       12       12       70
0.037949 0.028564        65536        65536.0       11       11       67
0.030373 0.022797       131072       131072.0       13       13      190
0.025665 0.020958       262144       262144.0        5        5      181
0.021706 0.021706       524288       524288.0        5        5      172 h
0.019350 0.016995      1048576      1048576.0        4        4      112 h
0.017840 0.016329      2097152      2097152.0        6        6      145 h
0.017033 0.016227      4194304      4194304.0        6        6       64 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.016205 h
total feature number = 631539945
Run:{3,None,0.5}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.500000 0.250000           16           16.0        8        8      172
0.562500 0.625000           32           32.0        9        9      106
0.578125 0.593750           64           64.0       14       13       82
0.523438 0.468750          128          128.0        1        7       88
0.414062 0.304688          256          256.0        1       12      121
0.308594 0.203125          512          512.0        3        6      106
0.211914 0.115234         1024         1024.0       10       10      106
0.159180 0.106445         2048         2048.0        8        8      166
0.104004 0.048828         4096         4096.0        8        8       91
0.078247 0.052490         8192         8192.0        3        3      139
0.059143 0.040039        16384        16384.0        5        5      112
0.046204 0.033264        32768        32768.0       12       12       70
0.036804 0.027405        65536        65536.0       11       11       67
0.029465 0.022125       131072       131072.0       13       13      190
0.024811 0.020157       262144       262144.0        5        5      181
0.021023 0.021023       524288       524288.0        5        5      172 h
0.018808 0.016594      1048576      1048576.0        4        4      112 h
0.017503 0.016197      2097152      2097152.0        6        6      145 h

finished run
number of examples per pass = 373334
passes used = 11
weighted example sum = 4106674.000000
weighted label sum = 0.000000
average loss = 0.016109 h
total feature number = 463129293
Run:{3,None,0.7}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      169
1.000000 1.000000            2            2.0        9        3       43
0.750000 0.500000            4            4.0        9        9      130
0.750000 0.750000            8            8.0        8        8       13
0.500000 0.250000           16           16.0        8        8      172
0.562500 0.625000           32           32.0        9        9      106
0.578125 0.593750           64           64.0       14       13       82
0.523438 0.468750          128          128.0        1        7       88
0.414062 0.304688          256          256.0        1       12      121
0.306641 0.199219          512          512.0        3        6      106
0.210938 0.115234         1024         1024.0       10       10      106
0.158203 0.105469         2048         2048.0        8        8      166
0.103516 0.048828         4096         4096.0        8        8       91
0.077881 0.052246         8192         8192.0        3        3      139
0.058655 0.039429        16384        16384.0        5        5      112
0.045502 0.032349        32768        32768.0       12       12       70
0.036194 0.026886        65536        65536.0       11       11       67
0.028946 0.021698       131072       131072.0       13       13      190
0.024334 0.019722       262144       262144.0        5        5      181
0.020668 0.020668       524288       524288.0        5        5      172 h
0.018574 0.016480      1048576      1048576.0        4        4      112 h
0.017354 0.016134      2097152      2097152.0        6        6      145 h
0.016716 0.016078      4194304      4194304.0        6        6       64 h

finished run
number of examples per pass = 373334
passes used = 13
weighted example sum = 4853342.000000
weighted label sum = 0.000000
average loss = 0.016066 h
total feature number = 547334619
Run:{3,None,0.9}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      332
1.000000 1.000000            2            2.0        9        3       80
0.750000 0.500000            4            4.0        9        9      254
0.750000 0.750000            8            8.0        8        8       20
0.562500 0.375000           16           16.0        8       13      338
0.593750 0.625000           32           32.0        9        9      206
0.625000 0.656250           64           64.0       14       13      158
0.601562 0.578125          128          128.0        1        7      170
0.523438 0.445312          256          256.0        1       12      236
0.404297 0.285156          512          512.0        3        2      206
0.322266 0.240234         1024         1024.0       10       10      206
0.254395 0.186523         2048         2048.0        8        8      326
0.179932 0.105469         4096         4096.0        8        8      176
0.141968 0.104004         8192         8192.0        3        5      272
0.117737 0.093506        16384        16384.0        5        5      218
0.101776 0.085815        32768        32768.0       12       12      134
0.091492 0.081207        65536        65536.0       11       11      128
0.084335 0.077179       131072       131072.0       13       13      374
0.081493 0.078651       262144       262144.0        5        5      356
0.077122 0.077122       524288       524288.0        5        5      338 h
0.073990 0.070858      1048576      1048576.0        4        4      218 h
0.070749 0.067507      2097152      2097152.0        6        6      284 h
0.067514 0.064280      4194304      4194304.0        6        6      122 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.061790 h
total feature number = 1229479980
Run:{3,1,0.001}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      332
1.000000 1.000000            2            2.0        9        3       80
0.750000 0.500000            4            4.0        9        9      254
0.750000 0.750000            8            8.0        8        8       20
0.562500 0.375000           16           16.0        8       13      338
0.593750 0.625000           32           32.0        9        9      206
0.609375 0.625000           64           64.0       14       13      158
0.593750 0.578125          128          128.0        1        7      170
0.519531 0.445312          256          256.0        1       12      236
0.402344 0.285156          512          512.0        3        2      206
0.321289 0.240234         1024         1024.0       10       10      206
0.248535 0.175781         2048         2048.0        8        8      326
0.172607 0.096680         4096         4096.0        8        8      176
0.134644 0.096680         8192         8192.0        3        5      272
0.109741 0.084839        16384        16384.0        5        5      218
0.092651 0.075562        32768        32768.0       12       12      134
0.081039 0.069427        65536        65536.0       11       11      128
0.071785 0.062531       131072       131072.0       13       13      374
0.066166 0.060547       262144       262144.0        5        5      356
0.058819 0.058819       524288       524288.0        5        5      338 h
0.052862 0.046906      1048576      1048576.0        4        4      218 h
0.046932 0.041003      2097152      2097152.0        6        6      284 h
0.041593 0.036254      4194304      4194304.0        6        6      122 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.032888 h
total feature number = 1229479980
Run:{3,1,0.01}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      332
1.000000 1.000000            2            2.0        9        3       80
0.750000 0.500000            4            4.0        9        9      254
0.750000 0.750000            8            8.0        8        8       20
0.562500 0.375000           16           16.0        8       13      338
0.593750 0.625000           32           32.0        9        9      206
0.593750 0.593750           64           64.0       14       13      158
0.546875 0.500000          128          128.0        1        7      170
0.464844 0.382812          256          256.0        1       12      236
0.365234 0.265625          512          512.0        3        2      206
0.270508 0.175781         1024         1024.0       10       10      206
0.204102 0.137695         2048         2048.0        8        8      326
0.139893 0.075684         4096         4096.0        8        8      176
0.104858 0.069824         8192         8192.0        3        5      272
0.083557 0.062256        16384        16384.0        5        5      218
0.065979 0.048401        32768        32768.0       12       12      134
0.054337 0.042694        65536        65536.0       11       11      128
0.044312 0.034286       131072       131072.0       13       13      374
0.037300 0.030289       262144       262144.0        5        5      356
0.031414 0.031414       524288       524288.0        5        5      338 h
0.027491 0.023567      1048576      1048576.0        4        4      218 h
0.024498 0.021505      2097152      2097152.0        6        6      284 h
0.022407 0.020315      4194304      4194304.0        6        6      122 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.019554 h
total feature number = 1229479980
Run:{3,1,0.1}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      332
1.000000 1.000000            2            2.0        9        3       80
0.750000 0.500000            4            4.0        9        9      254
0.750000 0.750000            8            8.0        8        8       20
0.562500 0.375000           16           16.0        8       13      338
0.593750 0.625000           32           32.0        9        9      206
0.562500 0.531250           64           64.0       14       13      158
0.507812 0.453125          128          128.0        1        7      170
0.425781 0.343750          256          256.0        1       12      236
0.333984 0.242188          512          512.0        3        2      206
0.241211 0.148438         1024         1024.0       10       10      206
0.181641 0.122070         2048         2048.0        8        8      326
0.120850 0.060059         4096         4096.0        8        8      176
0.091064 0.061279         8192         8192.0        3        3      272
0.070740 0.050415        16384        16384.0        5        5      218
0.055389 0.040039        32768        32768.0       12       12      134
0.044800 0.034210        65536        65536.0       11       11      128
0.036133 0.027466       131072       131072.0       13       13      374
0.030212 0.024292       262144       262144.0        5        5      356
0.025788 0.025788       524288       524288.0        5        5      338 h
0.022787 0.019787      1048576      1048576.0        4        4      218 h
0.020834 0.018881      2097152      2097152.0        6        6      284 h
0.019730 0.018625      4194304      4194304.0        6        6      122 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.018520 h
total feature number = 1229479980
Run:{3,1,0.3}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      332
1.000000 1.000000            2            2.0        9        3       80
0.750000 0.500000            4            4.0        9        9      254
0.750000 0.750000            8            8.0        8        8       20
0.500000 0.250000           16           16.0        8        8      338
0.562500 0.625000           32           32.0        9        9      206
0.546875 0.531250           64           64.0       14       13      158
0.500000 0.453125          128          128.0        1        7      170
0.414062 0.328125          256          256.0        1       12      236
0.326172 0.238281          512          512.0        3        2      206
0.230469 0.134766         1024         1024.0       10       10      206
0.174805 0.119141         2048         2048.0        8        8      326
0.116699 0.058594         4096         4096.0        8        8      176
0.087891 0.059082         8192         8192.0        3        3      272
0.066772 0.045654        16384        16384.0        5        5      218
0.052368 0.037964        32768        32768.0       12       12      134
0.042328 0.032288        65536        65536.0       11       11      128
0.034081 0.025833       131072       131072.0       13       13      374
0.028507 0.022934       262144       262144.0        5        5      356
0.024323 0.024323       524288       524288.0        5        5      338 h
0.021608 0.018894      1048576      1048576.0        4        4      218 h
0.020004 0.018400      2097152      2097152.0        6        6      284 h
0.019135 0.018266      4194304      4194304.0        6        6      122 h

finished run
number of examples per pass = 373334
passes used = 15
weighted example sum = 5600010.000000
weighted label sum = 0.000000
average loss = 0.018177 h
total feature number = 1229479980
Run:{3,1,0.5}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      332
1.000000 1.000000            2            2.0        9        3       80
0.750000 0.500000            4            4.0        9        9      254
0.750000 0.750000            8            8.0        8        8       20
0.500000 0.250000           16           16.0        8        8      338
0.562500 0.625000           32           32.0        9        9      206
0.531250 0.500000           64           64.0       14       13      158
0.484375 0.437500          128          128.0        1        7      170
0.402344 0.320312          256          256.0        1       12      236
0.314453 0.226562          512          512.0        3        2      206
0.224609 0.134766         1024         1024.0       10       10      206
0.170898 0.117188         2048         2048.0        8        8      326
0.114502 0.058105         4096         4096.0        8        8      176
0.085693 0.056885         8192         8192.0        3        3      272
0.065186 0.044678        16384        16384.0        5        5      218
0.050995 0.036804        32768        32768.0       12       12      134
0.041443 0.031891        65536        65536.0       11       11      128
0.033379 0.025314       131072       131072.0       13       13      374
0.027832 0.022285       262144       262144.0        5        5      356
0.023705 0.023705       524288       524288.0        5        5      338 h
0.021221 0.018738      1048576      1048576.0        4        4      218 h
0.019755 0.018290      2097152      2097152.0        6        6      284 h

finished run
number of examples per pass = 373334
passes used = 9
weighted example sum = 3360006.000000
weighted label sum = 0.000000
average loss = 0.018246 h
total feature number = 737687988
Run:{3,1,0.7}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/dbpedia_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        3        1      332
1.000000 1.000000            2            2.0        9        3       80
0.750000 0.500000            4            4.0        9        9      254
0.750000 0.750000            8            8.0        8        8       20
0.500000 0.250000           16           16.0        8        8      338
0.562500 0.625000           32           32.0        9        9      206
0.531250 0.500000           64           64.0       14       13      158
0.484375 0.437500          128          128.0        1        7      170
0.402344 0.320312          256          256.0        1       12      236
0.312500 0.222656          512          512.0        3        2      206
0.220703 0.128906         1024         1024.0       10       10      206
0.165039 0.109375         2048         2048.0        8        8      326
0.110840 0.056641         4096         4096.0        8        8      176
0.083740 0.056641         8192         8192.0        3        3      272
0.063721 0.043701        16384        16384.0        5        5      218
0.050049 0.036377        32768        32768.0       12       12      134
0.040726 0.031403        65536        65536.0       11       11      128
0.032837 0.024948       131072       131072.0       13       13      374
0.027481 0.022125       262144       262144.0        5        5      356
0.023361 0.023361       524288       524288.0        5        5      338 h
0.021023 0.018684      1048576      1048576.0        4        4      218 h
0.019657 0.018292      2097152      2097152.0        6        6      284 h

finished run
number of examples per pass = 373334
passes used = 7
weighted example sum = 2613338.000000
weighted label sum = 0.000000
average loss = 0.018225 h
total feature number = 573757324
Run:{3,1,0.9}


