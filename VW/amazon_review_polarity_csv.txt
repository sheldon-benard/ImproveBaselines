Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.500000 0.500000            8            8.0        2        2       36
0.437500 0.375000           16           16.0        2        2       56
0.531250 0.625000           32           32.0        1        2       77
0.500000 0.468750           64           64.0        1        2       41
0.429688 0.359375          128          128.0        1        1      125
0.449219 0.468750          256          256.0        1        1      131
0.427734 0.406250          512          512.0        2        1       27
0.355469 0.283203         1024         1024.0        1        1       47
0.309570 0.263672         2048         2048.0        2        1       42
0.287354 0.265137         4096         4096.0        2        1       24
0.270264 0.253174         8192         8192.0        1        1       21
0.262146 0.254028        16384        16384.0        2        2       67
0.266022 0.269897        32768        32768.0        2        1       20
0.268234 0.270447        65536        65536.0        1        1       34
0.253784 0.239334       131072       131072.0        2        2       70
0.239273 0.224762       262144       262144.0        1        1      122
0.224167 0.209061       524288       524288.0        2        2       88
0.209055 0.193943      1048576      1048576.0        2        1       53
0.196651 0.184248      2097152      2097152.0        1        1      110
0.187184 0.187184      4194304      4194304.0        1        1       84 h
0.179061 0.170938      8388608      8388608.0        2        2       48 h
0.171605 0.164149     16777216     16777216.0        2        2       38 h
0.164116 0.156627     33554432     33554432.0        2        1      115 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.152704 h
total feature number = 2238852810
Run:{1,None,0.001}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.500000 0.500000            8            8.0        2        2       36
0.437500 0.375000           16           16.0        2        2       56
0.531250 0.625000           32           32.0        1        2       77
0.468750 0.406250           64           64.0        1        2       41
0.421875 0.375000          128          128.0        1        1      125
0.445312 0.468750          256          256.0        1        1      131
0.408203 0.371094          512          512.0        2        1       27
0.328125 0.248047         1024         1024.0        1        1       47
0.274414 0.220703         2048         2048.0        2        1       42
0.246094 0.217773         4096         4096.0        2        1       24
0.219116 0.192139         8192         8192.0        1        1       21
0.203491 0.187866        16384        16384.0        2        2       67
0.192505 0.181519        32768        32768.0        2        2       20
0.184860 0.177216        65536        65536.0        1        1       34
0.177437 0.170013       131072       131072.0        2        2       70
0.169502 0.161568       262144       262144.0        1        1      122
0.161020 0.152538       524288       524288.0        2        2       88
0.152999 0.144978      1048576      1048576.0        2        1       53
0.145039 0.137078      2097152      2097152.0        1        1      110
0.137410 0.137410      4194304      4194304.0        1        1       84 h
0.129709 0.122008      8388608      8388608.0        2        2       48 h
0.122698 0.115687     16777216     16777216.0        2        2       38 h
0.116535 0.110371     33554432     33554432.0        2        1      115 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.108019 h
total feature number = 2238852810
Run:{1,None,0.01}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.500000 0.500000            8            8.0        2        2       36
0.437500 0.375000           16           16.0        2        2       56
0.500000 0.562500           32           32.0        1        2       77
0.453125 0.406250           64           64.0        1        2       41
0.398438 0.343750          128          128.0        1        1      125
0.398438 0.398438          256          256.0        1        2      131
0.332031 0.265625          512          512.0        2        1       27
0.283203 0.234375         1024         1024.0        1        1       47
0.232422 0.181641         2048         2048.0        2        2       42
0.207764 0.183105         4096         4096.0        2        2       24
0.182251 0.156738         8192         8192.0        1        1       21
0.169617 0.156982        16384        16384.0        2        2       67
0.155121 0.140625        32768        32768.0        2        2       20
0.143539 0.131958        65536        65536.0        1        1       34
0.134079 0.124619       131072       131072.0        2        2       70
0.123959 0.113838       262144       262144.0        1        1      122
0.116213 0.108467       524288       524288.0        2        2       88
0.110312 0.104410      1048576      1048576.0        2        1       53
0.105295 0.100278      2097152      2097152.0        1        1      110
0.101370 0.101370      4194304      4194304.0        1        1       84 h
0.098090 0.094809      8388608      8388608.0        2        2       48 h
0.095593 0.093096     16777216     16777216.0        2        2       38 h
0.093661 0.091730     33554432     33554432.0        2        1      115 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.091252 h
total feature number = 2238852810
Run:{1,None,0.1}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.500000 0.500000            8            8.0        2        2       36
0.437500 0.375000           16           16.0        2        2       56
0.375000 0.312500           32           32.0        1        1       77
0.375000 0.375000           64           64.0        1        2       41
0.375000 0.375000          128          128.0        1        1      125
0.359375 0.343750          256          256.0        1        2      131
0.314453 0.269531          512          512.0        2        1       27
0.261719 0.208984         1024         1024.0        1        1       47
0.215820 0.169922         2048         2048.0        2        2       42
0.188965 0.162109         4096         4096.0        2        2       24
0.165039 0.141113         8192         8192.0        1        1       21
0.151184 0.137329        16384        16384.0        2        2       67
0.137390 0.123596        32768        32768.0        2        2       20
0.128021 0.118652        65536        65536.0        1        1       34
0.119690 0.111359       131072       131072.0        2        2       70
0.111431 0.103172       262144       262144.0        1        2      122
0.105270 0.099110       524288       524288.0        2        2       88
0.100773 0.096275      1048576      1048576.0        2        1       53
0.097097 0.093421      2097152      2097152.0        1        1      110
0.094659 0.094659      4194304      4194304.0        1        1       84 h
0.092762 0.090866      8388608      8388608.0        2        2       48 h
0.091577 0.090391     16777216     16777216.0        2        2       38 h

finished run
number of examples per pass = 2400000
passes used = 9
weighted example sum = 21600000.000000
weighted label sum = 0.000000
average loss = 0.090330 h
total feature number = 1343311686
Run:{1,None,0.3}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.500000 0.500000            8            8.0        2        2       36
0.437500 0.375000           16           16.0        2        2       56
0.375000 0.312500           32           32.0        1        1       77
0.359375 0.343750           64           64.0        1        2       41
0.343750 0.328125          128          128.0        1        1      125
0.343750 0.343750          256          256.0        1        2      131
0.308594 0.273438          512          512.0        2        1       27
0.259766 0.210938         1024         1024.0        1        1       47
0.212402 0.165039         2048         2048.0        2        2       42
0.186768 0.161133         4096         4096.0        2        2       24
0.161499 0.136230         8192         8192.0        1        1       21
0.145813 0.130127        16384        16384.0        2        2       67
0.132477 0.119141        32768        32768.0        2        2       20
0.124374 0.116272        65536        65536.0        1        1       34
0.116386 0.108398       131072       131072.0        2        2       70
0.108711 0.101036       262144       262144.0        1        2      122
0.103020 0.097328       524288       524288.0        2        2       88
0.098998 0.094976      1048576      1048576.0        2        1       53
0.095587 0.092175      2097152      2097152.0        1        1      110
0.093432 0.093432      4194304      4194304.0        1        1       84 h
0.091949 0.090465      8388608      8388608.0        2        2       48 h
0.091226 0.090503     16777216     16777216.0        2        2       38 h

finished run
number of examples per pass = 2400000
passes used = 7
weighted example sum = 16800000.000000
weighted label sum = 0.000000
average loss = 0.090384 h
total feature number = 1044797978
Run:{1,None,0.5}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.625000 0.750000            8            8.0        2        1       36
0.500000 0.375000           16           16.0        2        2       56
0.437500 0.375000           32           32.0        1        2       77
0.390625 0.343750           64           64.0        1        2       41
0.359375 0.328125          128          128.0        1        1      125
0.351562 0.343750          256          256.0        1        2      131
0.310547 0.269531          512          512.0        2        1       27
0.259766 0.208984         1024         1024.0        1        1       47
0.213867 0.167969         2048         2048.0        2        2       42
0.187744 0.161621         4096         4096.0        2        2       24
0.161621 0.135498         8192         8192.0        1        1       21
0.145386 0.129150        16384        16384.0        2        2       67
0.131714 0.118042        32768        32768.0        2        2       20
0.123718 0.115723        65536        65536.0        1        1       34
0.116150 0.108582       131072       131072.0        2        2       70
0.108387 0.100624       262144       262144.0        1        2      122
0.102613 0.096840       524288       524288.0        2        2       88
0.098614 0.094614      1048576      1048576.0        2        1       53
0.095243 0.091871      2097152      2097152.0        1        2      110
0.093237 0.093237      4194304      4194304.0        1        1       84 h
0.091930 0.090622      8388608      8388608.0        2        2       48 h

finished run
number of examples per pass = 2400000
passes used = 6
weighted example sum = 14400000.000000
weighted label sum = 0.000000
average loss = 0.090598 h
total feature number = 895541124
Run:{1,None,0.7}


Generating 1-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.625000 0.750000            8            8.0        2        1       36
0.500000 0.375000           16           16.0        2        2       56
0.437500 0.375000           32           32.0        1        2       77
0.375000 0.312500           64           64.0        1        2       41
0.359375 0.343750          128          128.0        1        1      125
0.332031 0.304688          256          256.0        1        2      131
0.294922 0.257812          512          512.0        2        1       27
0.249023 0.203125         1024         1024.0        1        1       47
0.204590 0.160156         2048         2048.0        2        2       42
0.182129 0.159668         4096         4096.0        2        2       24
0.157349 0.132568         8192         8192.0        1        1       21
0.142395 0.127441        16384        16384.0        2        2       67
0.130554 0.118713        32768        32768.0        2        2       20
0.123169 0.115784        65536        65536.0        1        1       34
0.116135 0.109100       131072       131072.0        2        2       70
0.108410 0.100685       262144       262144.0        1        2      122
0.102818 0.097225       524288       524288.0        2        2       88
0.098794 0.094770      1048576      1048576.0        2        2       53
0.095381 0.091968      2097152      2097152.0        1        2      110
0.093412 0.093412      4194304      4194304.0        1        1       84 h
0.092209 0.091005      8388608      8388608.0        2        2       48 h

finished run
number of examples per pass = 2400000
passes used = 6
weighted example sum = 14400000.000000
weighted label sum = 0.000000
average loss = 0.091008 h
total feature number = 895541124
Run:{1,None,0.9}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.500000 0.500000            8            8.0        2        2       36
0.437500 0.375000           16           16.0        2        2       56
0.531250 0.625000           32           32.0        1        2       77
0.500000 0.468750           64           64.0        1        2       41
0.429688 0.359375          128          128.0        1        1      125
0.449219 0.468750          256          256.0        1        1      131
0.427734 0.406250          512          512.0        2        1       27
0.355469 0.283203         1024         1024.0        1        1       47
0.309570 0.263672         2048         2048.0        2        1       42
0.287354 0.265137         4096         4096.0        2        1       24
0.270264 0.253174         8192         8192.0        1        1       21
0.262146 0.254028        16384        16384.0        2        2       67
0.266022 0.269897        32768        32768.0        2        1       20
0.268234 0.270447        65536        65536.0        1        1       34
0.253784 0.239334       131072       131072.0        2        2       70
0.239273 0.224762       262144       262144.0        1        1      122
0.224167 0.209061       524288       524288.0        2        2       88
0.209055 0.193943      1048576      1048576.0        2        1       53
0.196651 0.184248      2097152      2097152.0        1        1      110
0.187184 0.187184      4194304      4194304.0        1        1       84 h
0.179061 0.170938      8388608      8388608.0        2        2       48 h
0.171605 0.164149     16777216     16777216.0        2        2       38 h
0.164116 0.156627     33554432     33554432.0        2        1      115 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.152704 h
total feature number = 2238852810
Run:{1,1,0.001}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.500000 0.500000            8            8.0        2        2       36
0.437500 0.375000           16           16.0        2        2       56
0.531250 0.625000           32           32.0        1        2       77
0.468750 0.406250           64           64.0        1        2       41
0.421875 0.375000          128          128.0        1        1      125
0.445312 0.468750          256          256.0        1        1      131
0.408203 0.371094          512          512.0        2        1       27
0.328125 0.248047         1024         1024.0        1        1       47
0.274414 0.220703         2048         2048.0        2        1       42
0.246094 0.217773         4096         4096.0        2        1       24
0.219116 0.192139         8192         8192.0        1        1       21
0.203491 0.187866        16384        16384.0        2        2       67
0.192505 0.181519        32768        32768.0        2        2       20
0.184860 0.177216        65536        65536.0        1        1       34
0.177437 0.170013       131072       131072.0        2        2       70
0.169502 0.161568       262144       262144.0        1        1      122
0.161020 0.152538       524288       524288.0        2        2       88
0.152999 0.144978      1048576      1048576.0        2        1       53
0.145039 0.137078      2097152      2097152.0        1        1      110
0.137410 0.137410      4194304      4194304.0        1        1       84 h
0.129709 0.122008      8388608      8388608.0        2        2       48 h
0.122698 0.115687     16777216     16777216.0        2        2       38 h
0.116535 0.110371     33554432     33554432.0        2        1      115 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.108019 h
total feature number = 2238852810
Run:{1,1,0.01}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.500000 0.500000            8            8.0        2        2       36
0.437500 0.375000           16           16.0        2        2       56
0.500000 0.562500           32           32.0        1        2       77
0.453125 0.406250           64           64.0        1        2       41
0.398438 0.343750          128          128.0        1        1      125
0.398438 0.398438          256          256.0        1        2      131
0.332031 0.265625          512          512.0        2        1       27
0.283203 0.234375         1024         1024.0        1        1       47
0.232422 0.181641         2048         2048.0        2        2       42
0.207764 0.183105         4096         4096.0        2        2       24
0.182251 0.156738         8192         8192.0        1        1       21
0.169617 0.156982        16384        16384.0        2        2       67
0.155121 0.140625        32768        32768.0        2        2       20
0.143539 0.131958        65536        65536.0        1        1       34
0.134079 0.124619       131072       131072.0        2        2       70
0.123959 0.113838       262144       262144.0        1        1      122
0.116213 0.108467       524288       524288.0        2        2       88
0.110312 0.104410      1048576      1048576.0        2        1       53
0.105295 0.100278      2097152      2097152.0        1        1      110
0.101370 0.101370      4194304      4194304.0        1        1       84 h
0.098090 0.094809      8388608      8388608.0        2        2       48 h
0.095593 0.093096     16777216     16777216.0        2        2       38 h
0.093661 0.091730     33554432     33554432.0        2        1      115 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.091252 h
total feature number = 2238852810
Run:{1,1,0.1}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.500000 0.500000            8            8.0        2        2       36
0.437500 0.375000           16           16.0        2        2       56
0.375000 0.312500           32           32.0        1        1       77
0.375000 0.375000           64           64.0        1        2       41
0.375000 0.375000          128          128.0        1        1      125
0.359375 0.343750          256          256.0        1        2      131
0.314453 0.269531          512          512.0        2        1       27
0.261719 0.208984         1024         1024.0        1        1       47
0.215820 0.169922         2048         2048.0        2        2       42
0.188965 0.162109         4096         4096.0        2        2       24
0.165039 0.141113         8192         8192.0        1        1       21
0.151184 0.137329        16384        16384.0        2        2       67
0.137390 0.123596        32768        32768.0        2        2       20
0.128021 0.118652        65536        65536.0        1        1       34
0.119690 0.111359       131072       131072.0        2        2       70
0.111431 0.103172       262144       262144.0        1        2      122
0.105270 0.099110       524288       524288.0        2        2       88
0.100773 0.096275      1048576      1048576.0        2        1       53
0.097097 0.093421      2097152      2097152.0        1        1      110
0.094659 0.094659      4194304      4194304.0        1        1       84 h
0.092762 0.090866      8388608      8388608.0        2        2       48 h
0.091577 0.090391     16777216     16777216.0        2        2       38 h

finished run
number of examples per pass = 2400000
passes used = 9
weighted example sum = 21600000.000000
weighted label sum = 0.000000
average loss = 0.090330 h
total feature number = 1343311686
Run:{1,1,0.3}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.500000 0.500000            8            8.0        2        2       36
0.437500 0.375000           16           16.0        2        2       56
0.375000 0.312500           32           32.0        1        1       77
0.359375 0.343750           64           64.0        1        2       41
0.343750 0.328125          128          128.0        1        1      125
0.343750 0.343750          256          256.0        1        2      131
0.308594 0.273438          512          512.0        2        1       27
0.259766 0.210938         1024         1024.0        1        1       47
0.212402 0.165039         2048         2048.0        2        2       42
0.186768 0.161133         4096         4096.0        2        2       24
0.161499 0.136230         8192         8192.0        1        1       21
0.145813 0.130127        16384        16384.0        2        2       67
0.132477 0.119141        32768        32768.0        2        2       20
0.124374 0.116272        65536        65536.0        1        1       34
0.116386 0.108398       131072       131072.0        2        2       70
0.108711 0.101036       262144       262144.0        1        2      122
0.103020 0.097328       524288       524288.0        2        2       88
0.098998 0.094976      1048576      1048576.0        2        1       53
0.095587 0.092175      2097152      2097152.0        1        1      110
0.093432 0.093432      4194304      4194304.0        1        1       84 h
0.091949 0.090465      8388608      8388608.0        2        2       48 h
0.091226 0.090503     16777216     16777216.0        2        2       38 h

finished run
number of examples per pass = 2400000
passes used = 7
weighted example sum = 16800000.000000
weighted label sum = 0.000000
average loss = 0.090384 h
total feature number = 1044797978
Run:{1,1,0.5}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.625000 0.750000            8            8.0        2        1       36
0.500000 0.375000           16           16.0        2        2       56
0.437500 0.375000           32           32.0        1        2       77
0.390625 0.343750           64           64.0        1        2       41
0.359375 0.328125          128          128.0        1        1      125
0.351562 0.343750          256          256.0        1        2      131
0.310547 0.269531          512          512.0        2        1       27
0.259766 0.208984         1024         1024.0        1        1       47
0.213867 0.167969         2048         2048.0        2        2       42
0.187744 0.161621         4096         4096.0        2        2       24
0.161621 0.135498         8192         8192.0        1        1       21
0.145386 0.129150        16384        16384.0        2        2       67
0.131714 0.118042        32768        32768.0        2        2       20
0.123718 0.115723        65536        65536.0        1        1       34
0.116150 0.108582       131072       131072.0        2        2       70
0.108387 0.100624       262144       262144.0        1        2      122
0.102613 0.096840       524288       524288.0        2        2       88
0.098614 0.094614      1048576      1048576.0        2        1       53
0.095243 0.091871      2097152      2097152.0        1        2      110
0.093237 0.093237      4194304      4194304.0        1        1       84 h
0.091930 0.090622      8388608      8388608.0        2        2       48 h

finished run
number of examples per pass = 2400000
passes used = 6
weighted example sum = 14400000.000000
weighted label sum = 0.000000
average loss = 0.090598 h
total feature number = 895541124
Run:{1,1,0.7}


Generating 1-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1       98
0.500000 0.000000            2            2.0        2        2       40
0.500000 0.500000            4            4.0        1        2       56
0.625000 0.750000            8            8.0        2        1       36
0.500000 0.375000           16           16.0        2        2       56
0.437500 0.375000           32           32.0        1        2       77
0.375000 0.312500           64           64.0        1        2       41
0.359375 0.343750          128          128.0        1        1      125
0.332031 0.304688          256          256.0        1        2      131
0.294922 0.257812          512          512.0        2        1       27
0.249023 0.203125         1024         1024.0        1        1       47
0.204590 0.160156         2048         2048.0        2        2       42
0.182129 0.159668         4096         4096.0        2        2       24
0.157349 0.132568         8192         8192.0        1        1       21
0.142395 0.127441        16384        16384.0        2        2       67
0.130554 0.118713        32768        32768.0        2        2       20
0.123169 0.115784        65536        65536.0        1        1       34
0.116135 0.109100       131072       131072.0        2        2       70
0.108410 0.100685       262144       262144.0        1        2      122
0.102818 0.097225       524288       524288.0        2        2       88
0.098794 0.094770      1048576      1048576.0        2        2       53
0.095381 0.091968      2097152      2097152.0        1        2      110
0.093412 0.093412      4194304      4194304.0        1        1       84 h
0.092209 0.091005      8388608      8388608.0        2        2       48 h

finished run
number of examples per pass = 2400000
passes used = 6
weighted example sum = 14400000.000000
weighted label sum = 0.000000
average loss = 0.091008 h
total feature number = 895541124
Run:{1,1,0.9}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      194
0.500000 0.000000            2            2.0        2        2       78
0.500000 0.500000            4            4.0        1        2      110
0.500000 0.500000            8            8.0        2        2       70
0.437500 0.375000           16           16.0        2        2      110
0.531250 0.625000           32           32.0        1        2      152
0.500000 0.468750           64           64.0        1        2       80
0.421875 0.343750          128          128.0        1        1      248
0.449219 0.476562          256          256.0        1        1      260
0.431641 0.414062          512          512.0        2        1       52
0.347656 0.263672         1024         1024.0        1        1       92
0.295898 0.244141         2048         2048.0        2        2       82
0.268799 0.241699         4096         4096.0        2        1       46
0.248535 0.228271         8192         8192.0        1        1       40
0.238770 0.229004        16384        16384.0        2        2      132
0.240875 0.242981        32768        32768.0        2        1       38
0.243927 0.246979        65536        65536.0        1        1       66
0.231102 0.218277       131072       131072.0        2        2      138
0.217907 0.204712       262144       262144.0        1        1      242
0.204460 0.191013       524288       524288.0        2        2      174
0.190523 0.176586      1048576      1048576.0        2        1      104
0.178342 0.166161      2097152      2097152.0        1        1      218
0.168783 0.168783      4194304      4194304.0        1        1      166 h
0.160512 0.152240      8388608      8388608.0        2        2       94 h
0.153339 0.146167     16777216     16777216.0        2        2       74 h
0.146376 0.139412     33554432     33554432.0        2        1      228 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.135906 h
total feature number = 4405705650
Run:{2,None,0.001}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      194
0.500000 0.000000            2            2.0        2        2       78
0.500000 0.500000            4            4.0        1        2      110
0.500000 0.500000            8            8.0        2        2       70
0.437500 0.375000           16           16.0        2        2      110
0.531250 0.625000           32           32.0        1        2      152
0.468750 0.406250           64           64.0        1        2       80
0.414062 0.359375          128          128.0        1        1      248
0.449219 0.484375          256          256.0        1        1      260
0.412109 0.375000          512          512.0        2        1       52
0.328125 0.244141         1024         1024.0        1        1       92
0.267578 0.207031         2048         2048.0        2        2       82
0.233398 0.199219         4096         4096.0        2        1       46
0.204224 0.175049         8192         8192.0        1        1       40
0.188049 0.171875        16384        16384.0        2        2      132
0.177002 0.165955        32768        32768.0        2        2       38
0.168686 0.160370        65536        65536.0        1        1       66
0.160927 0.153168       131072       131072.0        2        2      138
0.152210 0.143494       262144       262144.0        1        1      242
0.143686 0.135162       524288       524288.0        2        2      174
0.135761 0.127836      1048576      1048576.0        2        2      104
0.127535 0.119309      2097152      2097152.0        1        1      218
0.119551 0.119551      4194304      4194304.0        1        1      166 h
0.111195 0.102839      8388608      8388608.0        2        2       94 h
0.103338 0.095480     16777216     16777216.0        2        2       74 h
0.096087 0.088837     33554432     33554432.0        2        1      228 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.085712 h
total feature number = 4405705650
Run:{2,None,0.01}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      194
0.500000 0.000000            2            2.0        2        2       78
0.500000 0.500000            4            4.0        1        2      110
0.500000 0.500000            8            8.0        2        2       70
0.437500 0.375000           16           16.0        2        2      110
0.500000 0.562500           32           32.0        1        2      152
0.421875 0.343750           64           64.0        1        2       80
0.375000 0.328125          128          128.0        1        1      248
0.410156 0.445312          256          256.0        1        1      260
0.345703 0.281250          512          512.0        2        1       52
0.277344 0.208984         1024         1024.0        1        1       92
0.221191 0.165039         2048         2048.0        2        2       82
0.194092 0.166992         4096         4096.0        2        2       46
0.170166 0.146240         8192         8192.0        1        1       40
0.155579 0.140991        16384        16384.0        2        2      132
0.142242 0.128906        32768        32768.0        2        2       38
0.129684 0.117126        65536        65536.0        1        1       66
0.118057 0.106430       131072       131072.0        2        2      138
0.106346 0.094635       262144       262144.0        1        1      242
0.097179 0.088013       524288       524288.0        2        2      174
0.089474 0.081768      1048576      1048576.0        2        1      104
0.082675 0.075877      2097152      2097152.0        1        1      218
0.077169 0.077169      4194304      4194304.0        1        1      166 h
0.072341 0.067513      8388608      8388608.0        2        2       94 h
0.068417 0.064494     16777216     16777216.0        2        2       74 h
0.065333 0.062248     33554432     33554432.0        2        2      228 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.061356 h
total feature number = 4405705650
Run:{2,None,0.1}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      194
0.500000 0.000000            2            2.0        2        2       78
0.500000 0.500000            4            4.0        1        2      110
0.500000 0.500000            8            8.0        2        2       70
0.437500 0.375000           16           16.0        2        2      110
0.406250 0.375000           32           32.0        1        2      152
0.390625 0.375000           64           64.0        1        2       80
0.343750 0.296875          128          128.0        1        1      248
0.320312 0.296875          256          256.0        1        2      260
0.287109 0.253906          512          512.0        2        1       52
0.246094 0.205078         1024         1024.0        1        1       92
0.203613 0.161133         2048         2048.0        2        2       82
0.178223 0.152832         4096         4096.0        2        2       46
0.153442 0.128662         8192         8192.0        1        1       40
0.139404 0.125366        16384        16384.0        2        2      132
0.123962 0.108521        32768        32768.0        2        2       38
0.111938 0.099915        65536        65536.0        1        1       66
0.101738 0.091537       131072       131072.0        2        2      138
0.091351 0.080963       262144       262144.0        1        2      242
0.083282 0.075214       524288       524288.0        2        2      174
0.076653 0.070024      1048576      1048576.0        2        1      104
0.071002 0.065351      2097152      2097152.0        1        1      218
0.067003 0.067003      4194304      4194304.0        1        1      166 h
0.063870 0.060737      8388608      8388608.0        2        2       94 h
0.061883 0.059897     16777216     16777216.0        2        2       74 h

finished run
number of examples per pass = 2400000
passes used = 10
weighted example sum = 24000000.000000
weighted label sum = 0.000000
average loss = 0.059749 h
total feature number = 2937137100
Run:{2,None,0.3}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      194
0.500000 0.000000            2            2.0        2        2       78
0.500000 0.500000            4            4.0        1        2      110
0.500000 0.500000            8            8.0        2        2       70
0.437500 0.375000           16           16.0        2        2      110
0.375000 0.312500           32           32.0        1        2      152
0.375000 0.375000           64           64.0        1        2       80
0.335938 0.296875          128          128.0        1        1      248
0.312500 0.289062          256          256.0        1        2      260
0.279297 0.246094          512          512.0        2        1       52
0.238281 0.197266         1024         1024.0        1        1       92
0.199219 0.160156         2048         2048.0        2        2       82
0.173828 0.148438         4096         4096.0        2        2       46
0.148804 0.123779         8192         8192.0        1        1       40
0.134033 0.119263        16384        16384.0        2        2      132
0.119110 0.104187        32768        32768.0        2        2       38
0.106873 0.094635        65536        65536.0        1        1       66
0.097336 0.087799       131072       131072.0        2        2      138
0.087570 0.077805       262144       262144.0        1        2      242
0.079836 0.072102       524288       524288.0        2        2      174
0.073571 0.067307      1048576      1048576.0        2        1      104
0.068280 0.062988      2097152      2097152.0        1        1      218
0.064520 0.064520      4194304      4194304.0        1        1      166 h
0.062142 0.059763      8388608      8388608.0        2        2       94 h
0.061039 0.059936     16777216     16777216.0        2        2       74 h

finished run
number of examples per pass = 2400000
passes used = 7
weighted example sum = 16800000.000000
weighted label sum = 0.000000
average loss = 0.059686 h
total feature number = 2055995970
Run:{2,None,0.5}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      194
0.500000 0.000000            2            2.0        2        2       78
0.500000 0.500000            4            4.0        1        2      110
0.500000 0.500000            8            8.0        2        2       70
0.437500 0.375000           16           16.0        2        2      110
0.375000 0.312500           32           32.0        1        1      152
0.359375 0.343750           64           64.0        1        2       80
0.335938 0.312500          128          128.0        1        1      248
0.316406 0.296875          256          256.0        1        2      260
0.285156 0.253906          512          512.0        2        1       52
0.240234 0.195312         1024         1024.0        1        1       92
0.199707 0.159180         2048         2048.0        2        2       82
0.172607 0.145508         4096         4096.0        2        2       46
0.147705 0.122803         8192         8192.0        1        1       40
0.131531 0.115356        16384        16384.0        2        2      132
0.116791 0.102051        32768        32768.0        2        2       38
0.105164 0.093536        65536        65536.0        1        1       66
0.095718 0.086273       131072       131072.0        2        2      138
0.085979 0.076241       262144       262144.0        1        2      242
0.078497 0.071014       524288       524288.0        2        2      174
0.072389 0.066280      1048576      1048576.0        2        1      104
0.067279 0.062169      2097152      2097152.0        1        1      218
0.063705 0.063705      4194304      4194304.0        1        1      166 h
0.061782 0.059860      8388608      8388608.0        2        2       94 h

finished run
number of examples per pass = 2400000
passes used = 6
weighted example sum = 14400000.000000
weighted label sum = 0.000000
average loss = 0.059825 h
total feature number = 1762282260
Run:{2,None,0.7}


Generating 2-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      194
0.500000 0.000000            2            2.0        2        2       78
0.500000 0.500000            4            4.0        1        2      110
0.500000 0.500000            8            8.0        2        2       70
0.437500 0.375000           16           16.0        2        2      110
0.375000 0.312500           32           32.0        1        1      152
0.359375 0.343750           64           64.0        1        2       80
0.351562 0.343750          128          128.0        1        1      248
0.308594 0.265625          256          256.0        1        2      260
0.281250 0.253906          512          512.0        2        1       52
0.239258 0.197266         1024         1024.0        1        1       92
0.197266 0.155273         2048         2048.0        2        2       82
0.171875 0.146484         4096         4096.0        2        2       46
0.147583 0.123291         8192         8192.0        1        1       40
0.130554 0.113525        16384        16384.0        2        2      132
0.115875 0.101196        32768        32768.0        2        2       38
0.103928 0.091980        65536        65536.0        1        1       66
0.094437 0.084946       131072       131072.0        2        2      138
0.084957 0.075478       262144       262144.0        1        2      242
0.077824 0.070690       524288       524288.0        2        2      174
0.071886 0.065948      1048576      1048576.0        2        1      104
0.066947 0.062007      2097152      2097152.0        1        1      218
0.063503 0.063503      4194304      4194304.0        1        1      166 h
0.061886 0.060268      8388608      8388608.0        2        2       94 h

finished run
number of examples per pass = 2400000
passes used = 5
weighted example sum = 12000000.000000
weighted label sum = 0.000000
average loss = 0.060011 h
total feature number = 1468568550
Run:{2,None,0.9}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.531250 0.625000           32           32.0        1        2      226
0.500000 0.468750           64           64.0        1        2      118
0.414062 0.328125          128          128.0        1        1      370
0.445312 0.476562          256          256.0        1        1      388
0.429688 0.414062          512          512.0        2        1       76
0.342773 0.255859         1024         1024.0        1        1      136
0.289551 0.236328         2048         2048.0        2        2      121
0.261475 0.233398         4096         4096.0        2        1       67
0.243042 0.224609         8192         8192.0        1        1       58
0.232300 0.221558        16384        16384.0        2        2      196
0.233307 0.234314        32768        32768.0        2        1       55
0.235474 0.237640        65536        65536.0        1        1       97
0.224525 0.213577       131072       131072.0        2        2      205
0.212463 0.200401       262144       262144.0        1        1      361
0.199923 0.187382       524288       524288.0        2        2      259
0.187026 0.174129      1048576      1048576.0        2        1      154
0.175298 0.163570      2097152      2097152.0        1        1      325
0.165757 0.165757      4194304      4194304.0        1        1      247 h
0.157652 0.149548      8388608      8388608.0        2        2      139 h
0.150563 0.143475     16777216     16777216.0        2        2      109 h
0.143679 0.136795     33554432     33554432.0        2        1      340 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.133332 h
total feature number = 6536558535
Run:{2,1,0.001}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.531250 0.625000           32           32.0        1        2      226
0.484375 0.437500           64           64.0        1        2      118
0.429688 0.375000          128          128.0        1        1      370
0.453125 0.476562          256          256.0        1        1      388
0.408203 0.363281          512          512.0        2        1       76
0.324219 0.240234         1024         1024.0        1        1      136
0.266113 0.208008         2048         2048.0        2        2      121
0.231934 0.197754         4096         4096.0        2        1       67
0.203857 0.175781         8192         8192.0        1        1       58
0.187378 0.170898        16384        16384.0        2        2      196
0.176025 0.164673        32768        32768.0        2        2       55
0.167252 0.158478        65536        65536.0        1        1       97
0.159088 0.150925       131072       131072.0        2        2      205
0.150368 0.141647       262144       262144.0        1        1      361
0.141602 0.132835       524288       524288.0        2        2      259
0.133618 0.125635      1048576      1048576.0        2        1      154
0.125282 0.116945      2097152      2097152.0        1        1      325
0.117183 0.117183      4194304      4194304.0        1        1      247 h
0.108799 0.100414      8388608      8388608.0        2        2      139 h
0.100935 0.093072     16777216     16777216.0        2        2      109 h
0.093684 0.086432     33554432     33554432.0        2        1      340 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.083352 h
total feature number = 6536558535
Run:{2,1,0.01}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.531250 0.625000           32           32.0        1        2      226
0.468750 0.406250           64           64.0        1        2      118
0.414062 0.359375          128          128.0        1        1      370
0.437500 0.460938          256          256.0        1        1      388
0.353516 0.269531          512          512.0        2        1       76
0.285156 0.216797         1024         1024.0        1        1      136
0.226074 0.166992         2048         2048.0        2        2      121
0.194580 0.163086         4096         4096.0        2        2       67
0.169189 0.143799         8192         8192.0        1        1       58
0.155579 0.141968        16384        16384.0        2        2      196
0.141022 0.126465        32768        32768.0        2        2       55
0.128159 0.115295        65536        65536.0        1        1       97
0.116928 0.105698       131072       131072.0        2        2      205
0.105389 0.093849       262144       262144.0        1        1      361
0.095924 0.086460       524288       524288.0        2        2      259
0.087801 0.079678      1048576      1048576.0        2        1      154
0.080803 0.073805      2097152      2097152.0        1        1      325
0.075331 0.075331      4194304      4194304.0        1        1      247 h
0.070507 0.065683      8388608      8388608.0        2        2      139 h
0.066624 0.062741     16777216     16777216.0        2        2      109 h
0.063669 0.060714     33554432     33554432.0        2        2      340 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.059943 h
total feature number = 6536558535
Run:{2,1,0.1}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.437500 0.437500           32           32.0        1        2      226
0.406250 0.375000           64           64.0        1        2      118
0.351562 0.296875          128          128.0        1        1      370
0.347656 0.343750          256          256.0        1        2      388
0.300781 0.253906          512          512.0        2        1       76
0.255859 0.210938         1024         1024.0        1        1      136
0.203125 0.150391         2048         2048.0        2        2      121
0.175293 0.147461         4096         4096.0        2        2       67
0.151367 0.127441         8192         8192.0        1        1       58
0.138672 0.125977        16384        16384.0        2        2      196
0.123138 0.107605        32768        32768.0        2        2       55
0.111008 0.098877        65536        65536.0        1        1       97
0.101250 0.091492       131072       131072.0        2        2      205
0.090740 0.080231       262144       262144.0        1        2      361
0.082560 0.074379       524288       524288.0        2        2      259
0.075607 0.068655      1048576      1048576.0        2        1      154
0.069782 0.063957      2097152      2097152.0        1        1      325
0.065695 0.065695      4194304      4194304.0        1        1      247 h
0.062582 0.059469      8388608      8388608.0        2        2      139 h
0.060690 0.058799     16777216     16777216.0        2        2      109 h

finished run
number of examples per pass = 2400000
passes used = 10
weighted example sum = 24000000.000000
weighted label sum = 0.000000
average loss = 0.058653 h
total feature number = 4357705690
Run:{2,1,0.3}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.437500 0.437500           32           32.0        1        2      226
0.406250 0.375000           64           64.0        1        2      118
0.343750 0.281250          128          128.0        1        1      370
0.320312 0.296875          256          256.0        1        2      388
0.289062 0.257812          512          512.0        2        1       76
0.246094 0.203125         1024         1024.0        1        1      136
0.197266 0.148438         2048         2048.0        2        2      121
0.171143 0.145020         4096         4096.0        2        2       67
0.147949 0.124756         8192         8192.0        1        1       58
0.134033 0.120117        16384        16384.0        2        2      196
0.119293 0.104553        32768        32768.0        2        2       55
0.106949 0.094604        65536        65536.0        1        1       97
0.097313 0.087677       131072       131072.0        2        2      205
0.087132 0.076950       262144       262144.0        1        2      361
0.079464 0.071796       524288       524288.0        2        2      259
0.072793 0.066122      1048576      1048576.0        2        1      154
0.067307 0.061821      2097152      2097152.0        1        1      325
0.063561 0.063561      4194304      4194304.0        1        1      247 h
0.061147 0.058733      8388608      8388608.0        2        2      139 h
0.060043 0.058938     16777216     16777216.0        2        2      109 h

finished run
number of examples per pass = 2400000
passes used = 7
weighted example sum = 16800000.000000
weighted label sum = 0.000000
average loss = 0.058772 h
total feature number = 3050393983
Run:{2,1,0.5}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.375000 0.312500           32           32.0        1        2      226
0.359375 0.343750           64           64.0        1        2      118
0.343750 0.328125          128          128.0        1        1      370
0.308594 0.273438          256          256.0        1        2      388
0.281250 0.253906          512          512.0        2        1       76
0.239258 0.197266         1024         1024.0        1        1      136
0.191895 0.144531         2048         2048.0        2        2      121
0.170166 0.148438         4096         4096.0        2        2       67
0.147095 0.124023         8192         8192.0        1        1       58
0.132385 0.117676        16384        16384.0        2        2      196
0.116913 0.101440        32768        32768.0        2        2       55
0.104904 0.092896        65536        65536.0        1        1       97
0.095581 0.086258       131072       131072.0        2        2      205
0.085545 0.075508       262144       262144.0        1        2      361
0.078112 0.070679       524288       524288.0        2        2      259
0.071684 0.065256      1048576      1048576.0        2        1      154
0.066351 0.061019      2097152      2097152.0        1        1      325
0.062905 0.062905      4194304      4194304.0        1        1      247 h
0.060892 0.058879      8388608      8388608.0        2        2      139 h

finished run
number of examples per pass = 2400000
passes used = 6
weighted example sum = 14400000.000000
weighted label sum = 0.000000
average loss = 0.058926 h
total feature number = 2614623414
Run:{2,1,0.7}


Generating 2-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.375000 0.312500           32           32.0        1        2      226
0.359375 0.343750           64           64.0        1        2      118
0.351562 0.343750          128          128.0        1        1      370
0.308594 0.265625          256          256.0        1        2      388
0.281250 0.253906          512          512.0        2        1       76
0.240234 0.199219         1024         1024.0        1        1      136
0.193359 0.146484         2048         2048.0        2        2      121
0.171143 0.148926         4096         4096.0        2        2       67
0.147949 0.124756         8192         8192.0        1        1       58
0.131897 0.115845        16384        16384.0        2        2      196
0.116241 0.100586        32768        32768.0        2        2       55
0.104248 0.092255        65536        65536.0        1        1       97
0.094688 0.085129       131072       131072.0        2        2      205
0.084862 0.075035       262144       262144.0        1        2      361
0.077576 0.070290       524288       524288.0        2        2      259
0.071301 0.065027      1048576      1048576.0        2        1      154
0.066100 0.060898      2097152      2097152.0        1        1      325
0.062735 0.062735      4194304      4194304.0        1        1      247 h
0.060977 0.059219      8388608      8388608.0        2        2      139 h

finished run
number of examples per pass = 2400000
passes used = 5
weighted example sum = 12000000.000000
weighted label sum = 0.000000
average loss = 0.059115 h
total feature number = 2178852845
Run:{2,1,0.9}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.531250 0.625000           32           32.0        1        2      226
0.500000 0.468750           64           64.0        1        2      118
0.421875 0.343750          128          128.0        1        1      370
0.449219 0.476562          256          256.0        1        1      388
0.433594 0.417969          512          512.0        2        1       76
0.345703 0.257812         1024         1024.0        1        1      136
0.292969 0.240234         2048         2048.0        2        2      121
0.264648 0.236328         4096         4096.0        2        1       67
0.245239 0.225830         8192         8192.0        1        1       58
0.235596 0.225952        16384        16384.0        2        2      196
0.237823 0.240051        32768        32768.0        2        1       55
0.241928 0.246033        65536        65536.0        1        1       97
0.230545 0.219162       131072       131072.0        2        2      205
0.218170 0.205795       262144       262144.0        1        1      361
0.204994 0.191818       524288       524288.0        2        2      259
0.190772 0.176550      1048576      1048576.0        2        1      154
0.177631 0.164491      2097152      2097152.0        1        1      325
0.167053 0.167053      4194304      4194304.0        1        1      247 h
0.158059 0.149065      8388608      8388608.0        2        2      139 h
0.150605 0.143150     16777216     16777216.0        2        2      109 h
0.143756 0.136908     33554432     33554432.0        2        1      340 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.133587 h
total feature number = 6536558535
Run:{3,None,0.001}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.531250 0.625000           32           32.0        1        2      226
0.468750 0.406250           64           64.0        1        2      118
0.406250 0.343750          128          128.0        1        1      370
0.441406 0.476562          256          256.0        1        1      388
0.404297 0.367188          512          512.0        2        1       76
0.319336 0.234375         1024         1024.0        1        1      136
0.260254 0.201172         2048         2048.0        2        2      121
0.228516 0.196777         4096         4096.0        2        1       67
0.201050 0.173584         8192         8192.0        1        1       58
0.185791 0.170532        16384        16384.0        2        2      196
0.175049 0.164307        32768        32768.0        2        2       55
0.167038 0.159027        65536        65536.0        1        1       97
0.158859 0.150681       131072       131072.0        2        2      205
0.150223 0.141586       262144       262144.0        1        1      361
0.141737 0.133251       524288       524288.0        2        2      259
0.133986 0.126234      1048576      1048576.0        2        2      154
0.125898 0.117810      2097152      2097152.0        1        1      325
0.118084 0.118084      4194304      4194304.0        1        1      247 h
0.109751 0.101419      8388608      8388608.0        2        2      139 h
0.101814 0.093877     16777216     16777216.0        2        2      109 h
0.094445 0.087075     33554432     33554432.0        2        1      340 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.084013 h
total feature number = 6536558535
Run:{3,None,0.01}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.531250 0.625000           32           32.0        1        2      226
0.453125 0.375000           64           64.0        1        2      118
0.390625 0.328125          128          128.0        1        1      370
0.414062 0.437500          256          256.0        1        1      388
0.343750 0.273438          512          512.0        2        1       76
0.279297 0.214844         1024         1024.0        1        1      136
0.223633 0.167969         2048         2048.0        2        2      121
0.195801 0.167969         4096         4096.0        2        2       67
0.171875 0.147949         8192         8192.0        1        1       58
0.156494 0.141113        16384        16384.0        2        2      196
0.142426 0.128357        32768        32768.0        2        2       55
0.130066 0.117706        65536        65536.0        1        1       97
0.118721 0.107376       131072       131072.0        2        2      205
0.107033 0.095345       262144       262144.0        1        1      361
0.097301 0.087570       524288       524288.0        2        2      259
0.089020 0.080738      1048576      1048576.0        2        1      154
0.081787 0.074554      2097152      2097152.0        1        1      325
0.075857 0.075857      4194304      4194304.0        1        1      247 h
0.070905 0.065953      8388608      8388608.0        2        2      139 h
0.067008 0.063110     16777216     16777216.0        2        2      109 h
0.064102 0.061197     33554432     33554432.0        2        2      340 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.060494 h
total feature number = 6536558535
Run:{3,None,0.1}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.468750 0.500000           32           32.0        1        2      226
0.421875 0.375000           64           64.0        1        2      118
0.359375 0.296875          128          128.0        1        1      370
0.335938 0.312500          256          256.0        1        2      388
0.296875 0.257812          512          512.0        2        2       76
0.250977 0.205078         1024         1024.0        1        1      136
0.206055 0.161133         2048         2048.0        2        2      121
0.180420 0.154785         4096         4096.0        2        2       67
0.155273 0.130127         8192         8192.0        1        1       58
0.141846 0.128418        16384        16384.0        2        2      196
0.126160 0.110474        32768        32768.0        2        2       55
0.113892 0.101624        65536        65536.0        1        1       97
0.103012 0.092133       131072       131072.0        2        2      205
0.092182 0.081352       262144       262144.0        1        2      361
0.083635 0.075089       524288       524288.0        2        2      259
0.076508 0.069380      1048576      1048576.0        2        1      154
0.070476 0.064444      2097152      2097152.0        1        1      325
0.066092 0.066092      4194304      4194304.0        1        1      247 h
0.062969 0.059845      8388608      8388608.0        2        2      139 h
0.061101 0.059234     16777216     16777216.0        2        2      109 h

finished run
number of examples per pass = 2400000
passes used = 11
weighted example sum = 26400000.000000
weighted label sum = 0.000000
average loss = 0.059127 h
total feature number = 4793476259
Run:{3,None,0.3}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.437500 0.437500           32           32.0        1        2      226
0.406250 0.375000           64           64.0        1        2      118
0.351562 0.296875          128          128.0        1        1      370
0.324219 0.296875          256          256.0        1        2      388
0.289062 0.253906          512          512.0        2        2       76
0.244141 0.199219         1024         1024.0        1        1      136
0.201172 0.158203         2048         2048.0        2        2      121
0.176758 0.152344         4096         4096.0        2        2       67
0.151367 0.125977         8192         8192.0        1        1       58
0.137329 0.123291        16384        16384.0        2        2      196
0.121643 0.105957        32768        32768.0        2        2       55
0.108826 0.096008        65536        65536.0        1        1       97
0.098541 0.088257       131072       131072.0        2        2      205
0.088276 0.078011       262144       262144.0        1        2      361
0.080179 0.072083       524288       524288.0        2        2      259
0.073473 0.066767      1048576      1048576.0        2        2      154
0.067872 0.062270      2097152      2097152.0        1        1      325
0.063815 0.063815      4194304      4194304.0        1        1      247 h
0.061440 0.059066      8388608      8388608.0        2        2      139 h

finished run
number of examples per pass = 2400000
passes used = 6
weighted example sum = 14400000.000000
weighted label sum = 0.000000
average loss = 0.059073 h
total feature number = 2614623414
Run:{3,None,0.5}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.406250 0.375000           32           32.0        1        2      226
0.390625 0.375000           64           64.0        1        2      118
0.343750 0.296875          128          128.0        1        1      370
0.320312 0.296875          256          256.0        1        2      388
0.277344 0.234375          512          512.0        2        2       76
0.235352 0.193359         1024         1024.0        1        1      136
0.197754 0.160156         2048         2048.0        2        2      121
0.173584 0.149414         4096         4096.0        2        2       67
0.149414 0.125244         8192         8192.0        1        1       58
0.134705 0.119995        16384        16384.0        2        2      196
0.118805 0.102905        32768        32768.0        2        2       55
0.106339 0.093872        65536        65536.0        1        1       97
0.096375 0.086411       131072       131072.0        2        2      205
0.086487 0.076599       262144       262144.0        1        2      361
0.078701 0.070915       524288       524288.0        2        2      259
0.072217 0.065733      1048576      1048576.0        2        2      154
0.066840 0.061463      2097152      2097152.0        1        1      325
0.063080 0.063080      4194304      4194304.0        1        1      247 h
0.061100 0.059119      8388608      8388608.0        2        2      139 h

finished run
number of examples per pass = 2400000
passes used = 6
weighted example sum = 14400000.000000
weighted label sum = 0.000000
average loss = 0.059122 h
total feature number = 2614623414
Run:{3,None,0.7}


Generating 3-grams for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      289
0.500000 0.000000            2            2.0        2        2      115
0.500000 0.500000            4            4.0        1        2      163
0.500000 0.500000            8            8.0        2        2      103
0.437500 0.375000           16           16.0        2        2      163
0.375000 0.312500           32           32.0        1        2      226
0.359375 0.343750           64           64.0        1        2      118
0.328125 0.296875          128          128.0        1        1      370
0.304688 0.281250          256          256.0        1        2      388
0.267578 0.230469          512          512.0        2        2       76
0.229492 0.191406         1024         1024.0        1        1      136
0.195312 0.161133         2048         2048.0        2        2      121
0.172363 0.149414         4096         4096.0        2        2       67
0.147705 0.123047         8192         8192.0        1        1       58
0.133179 0.118652        16384        16384.0        2        2      196
0.117584 0.101990        32768        32768.0        2        2       55
0.105133 0.092682        65536        65536.0        1        1       97
0.095169 0.085205       131072       131072.0        2        2      205
0.085449 0.075729       262144       262144.0        1        2      361
0.077877 0.070305       524288       524288.0        2        2      259
0.071654 0.065432      1048576      1048576.0        2        2      154
0.066388 0.061121      2097152      2097152.0        1        1      325
0.062852 0.062852      4194304      4194304.0        1        1      247 h
0.061064 0.059276      8388608      8388608.0        2        2      139 h

finished run
number of examples per pass = 2400000
passes used = 6
weighted example sum = 14400000.000000
weighted label sum = 0.000000
average loss = 0.059267 h
total feature number = 2614623414
Run:{3,None,0.9}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.001
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      572
0.500000 0.000000            2            2.0        2        2      224
0.500000 0.500000            4            4.0        1        2      320
0.500000 0.500000            8            8.0        2        2      200
0.437500 0.375000           16           16.0        2        2      320
0.531250 0.625000           32           32.0        1        2      446
0.484375 0.437500           64           64.0        1        2      230
0.414062 0.343750          128          128.0        1        1      734
0.441406 0.468750          256          256.0        1        1      770
0.423828 0.406250          512          512.0        2        1      146
0.337891 0.251953         1024         1024.0        1        1      266
0.284180 0.230469         2048         2048.0        2        2      236
0.255371 0.226562         4096         4096.0        2        1      128
0.235840 0.216309         8192         8192.0        1        1      110
0.226440 0.217041        16384        16384.0        2        2      386
0.228271 0.230103        32768        32768.0        2        1      104
0.232864 0.237457        65536        65536.0        1        1      188
0.223557 0.214249       131072       131072.0        2        2      404
0.212681 0.201805       262144       262144.0        1        1      716
0.201309 0.189938       524288       524288.0        2        2      512
0.188462 0.175615      1048576      1048576.0        2        1      302
0.175667 0.162871      2097152      2097152.0        1        1      644
0.164629 0.164629      4194304      4194304.0        1        1      488 h
0.155217 0.145806      8388608      8388608.0        2        2      272 h
0.147663 0.140108     16777216     16777216.0        2        2      212 h
0.141074 0.134486     33554432     33554432.0        2        1      674 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.131588 h
total feature number = 12857117340
Run:{3,1,0.001}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.01
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      572
0.500000 0.000000            2            2.0        2        2      224
0.500000 0.500000            4            4.0        1        2      320
0.500000 0.500000            8            8.0        2        2      200
0.437500 0.375000           16           16.0        2        2      320
0.531250 0.625000           32           32.0        1        2      446
0.484375 0.437500           64           64.0        1        2      230
0.414062 0.343750          128          128.0        1        1      734
0.441406 0.468750          256          256.0        1        1      770
0.408203 0.375000          512          512.0        2        1      146
0.320312 0.232422         1024         1024.0        1        1      266
0.261230 0.202148         2048         2048.0        2        2      236
0.228516 0.195801         4096         4096.0        2        1      128
0.200806 0.173096         8192         8192.0        1        1      110
0.185303 0.169800        16384        16384.0        2        2      386
0.174713 0.164124        32768        32768.0        2        2      104
0.166000 0.157288        65536        65536.0        1        1      188
0.156784 0.147568       131072       131072.0        2        2      404
0.148193 0.139603       262144       262144.0        1        1      716
0.139692 0.131191       524288       524288.0        2        2      512
0.132267 0.124842      1048576      1048576.0        2        2      302
0.124354 0.116441      2097152      2097152.0        1        1      644
0.116735 0.116735      4194304      4194304.0        1        1      488 h
0.108578 0.100421      8388608      8388608.0        2        2      272 h
0.100748 0.092918     16777216     16777216.0        2        2      212 h
0.093433 0.086117     33554432     33554432.0        2        1      674 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.082956 h
total feature number = 12857117340
Run:{3,1,0.01}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.1
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      572
0.500000 0.000000            2            2.0        2        2      224
0.500000 0.500000            4            4.0        1        2      320
0.500000 0.500000            8            8.0        2        2      200
0.437500 0.375000           16           16.0        2        2      320
0.531250 0.625000           32           32.0        1        2      446
0.453125 0.375000           64           64.0        1        2      230
0.390625 0.328125          128          128.0        1        1      734
0.414062 0.437500          256          256.0        1        1      770
0.341797 0.269531          512          512.0        2        1      146
0.281250 0.220703         1024         1024.0        1        1      266
0.225098 0.168945         2048         2048.0        2        2      236
0.193359 0.161621         4096         4096.0        2        2      128
0.170654 0.147949         8192         8192.0        1        1      110
0.157043 0.143433        16384        16384.0        2        2      386
0.142456 0.127869        32768        32768.0        2        2      104
0.130478 0.118500        65536        65536.0        1        1      188
0.119797 0.109116       131072       131072.0        2        2      404
0.108406 0.097015       262144       262144.0        1        1      716
0.098648 0.088890       524288       524288.0        2        2      512
0.089848 0.081047      1048576      1048576.0        2        1      302
0.082186 0.074524      2097152      2097152.0        1        1      644
0.076236 0.076236      4194304      4194304.0        1        1      488 h
0.071269 0.066301      8388608      8388608.0        2        2      272 h
0.067503 0.063738     16777216     16777216.0        2        2      212 h
0.064736 0.061969     33554432     33554432.0        2        2      674 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.061334 h
total feature number = 12857117340
Run:{3,1,0.1}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.3
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      572
0.500000 0.000000            2            2.0        2        2      224
0.500000 0.500000            4            4.0        1        2      320
0.500000 0.500000            8            8.0        2        2      200
0.437500 0.375000           16           16.0        2        2      320
0.531250 0.625000           32           32.0        1        2      446
0.484375 0.437500           64           64.0        1        2      230
0.382812 0.281250          128          128.0        1        1      734
0.378906 0.375000          256          256.0        1        1      770
0.312500 0.246094          512          512.0        2        1      146
0.262695 0.212891         1024         1024.0        1        1      266
0.209961 0.157227         2048         2048.0        2        2      236
0.182373 0.154785         4096         4096.0        2        2      128
0.158081 0.133789         8192         8192.0        1        1      110
0.145752 0.133423        16384        16384.0        2        2      386
0.129547 0.113342        32768        32768.0        2        2      104
0.116974 0.104401        65536        65536.0        1        1      188
0.106705 0.096436       131072       131072.0        2        2      404
0.095459 0.084213       262144       262144.0        1        1      716
0.086386 0.077312       524288       524288.0        2        2      512
0.078642 0.070898      1048576      1048576.0        2        1      302
0.072005 0.065369      2097152      2097152.0        1        1      644
0.067445 0.067445      4194304      4194304.0        1        1      488 h
0.064240 0.061036      8388608      8388608.0        2        2      272 h
0.062336 0.060432     16777216     16777216.0        2        2      212 h
0.061291 0.060247     33554432     33554432.0        2        2      674 h

finished run
number of examples per pass = 2400000
passes used = 15
weighted example sum = 36000000.000000
weighted label sum = 0.000000
average loss = 0.060228 h
total feature number = 12857117340
Run:{3,1,0.3}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      572
0.500000 0.000000            2            2.0        2        2      224
0.500000 0.500000            4            4.0        1        2      320
0.500000 0.500000            8            8.0        2        2      200
0.437500 0.375000           16           16.0        2        2      320
0.500000 0.562500           32           32.0        1        2      446
0.453125 0.406250           64           64.0        1        2      230
0.375000 0.296875          128          128.0        1        1      734
0.343750 0.312500          256          256.0        1        2      770
0.298828 0.253906          512          512.0        2        1      146
0.253906 0.208984         1024         1024.0        1        1      266
0.203613 0.153320         2048         2048.0        2        2      236
0.176758 0.149902         4096         4096.0        2        2      128
0.154785 0.132812         8192         8192.0        1        1      110
0.141663 0.128540        16384        16384.0        2        2      386
0.125763 0.109863        32768        32768.0        2        2      104
0.113327 0.100891        65536        65536.0        1        1      188
0.102890 0.092453       131072       131072.0        2        2      404
0.092049 0.081207       262144       262144.0        1        1      716
0.083485 0.074921       524288       524288.0        2        2      512
0.075998 0.068512      1048576      1048576.0        2        1      302
0.069678 0.063357      2097152      2097152.0        1        1      644
0.065547 0.065547      4194304      4194304.0        1        1      488 h
0.062885 0.060223      8388608      8388608.0        2        2      272 h
0.061515 0.060145     16777216     16777216.0        2        2      212 h

finished run
number of examples per pass = 2400000
passes used = 8
weighted example sum = 19200000.000000
weighted label sum = 0.000000
average loss = 0.060127 h
total feature number = 6857129248
Run:{3,1,0.5}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.7
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      572
0.500000 0.000000            2            2.0        2        2      224
0.500000 0.500000            4            4.0        1        2      320
0.500000 0.500000            8            8.0        2        2      200
0.437500 0.375000           16           16.0        2        2      320
0.468750 0.500000           32           32.0        1        2      446
0.421875 0.375000           64           64.0        1        2      230
0.359375 0.296875          128          128.0        1        1      734
0.332031 0.304688          256          256.0        1        2      770
0.292969 0.253906          512          512.0        2        1      146
0.249023 0.205078         1024         1024.0        1        1      266
0.200684 0.152344         2048         2048.0        2        2      236
0.174805 0.148926         4096         4096.0        2        2      128
0.152466 0.130127         8192         8192.0        1        1      110
0.138916 0.125366        16384        16384.0        2        2      386
0.122955 0.106995        32768        32768.0        2        2      104
0.110916 0.098877        65536        65536.0        1        1      188
0.100929 0.090942       131072       131072.0        2        2      404
0.090290 0.079651       262144       262144.0        1        1      716
0.082014 0.073738       524288       524288.0        2        2      512
0.074850 0.067686      1048576      1048576.0        2        1      302
0.068782 0.062715      2097152      2097152.0        1        1      644
0.064831 0.064831      4194304      4194304.0        1        1      488 h
0.062464 0.060097      8388608      8388608.0        2        2      272 h
0.061320 0.060176     16777216     16777216.0        2        2      212 h

finished run
number of examples per pass = 2400000
passes used = 7
weighted example sum = 16800000.000000
weighted label sum = 0.000000
average loss = 0.060091 h
total feature number = 5999988092
Run:{3,1,0.7}


Generating 3-grams for all namespaces.
Generating 1-skips for all namespaces.
final_regressor = model.vw
Num weight bits = 25
learning rate = 0.9
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
using cache_file = ../data/amazon_review_polarity_csv/train_vw.csv.cache
ignoring text input in favor of cache input
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        2        1      572
0.500000 0.000000            2            2.0        2        2      224
0.500000 0.500000            4            4.0        1        2      320
0.500000 0.500000            8            8.0        2        2      200
0.437500 0.375000           16           16.0        2        2      320
0.437500 0.437500           32           32.0        1        2      446
0.406250 0.375000           64           64.0        1        2      230
0.343750 0.281250          128          128.0        1        1      734
0.312500 0.281250          256          256.0        1        2      770
0.283203 0.253906          512          512.0        2        1      146
0.246094 0.208984         1024         1024.0        1        1      266
0.200195 0.154297         2048         2048.0        2        2      236
0.174072 0.147949         4096         4096.0        2        2      128
0.151489 0.128906         8192         8192.0        1        1      110
0.137390 0.123291        16384        16384.0        2        2      386
0.121613 0.105835        32768        32768.0        2        2      104
0.109314 0.097015        65536        65536.0        1        1      188
0.099602 0.089890       131072       131072.0        2        2      404
0.089230 0.078857       262144       262144.0        1        1      716
0.081144 0.073059       524288       524288.0        2        2      512
0.074284 0.067423      1048576      1048576.0        2        1      302
0.068350 0.062416      2097152      2097152.0        1        1      644
0.064623 0.064623      4194304      4194304.0        1        1      488 h
0.062390 0.060156      8388608      8388608.0        2        2      272 h

finished run
number of examples per pass = 2400000
passes used = 6
weighted example sum = 14400000.000000
weighted label sum = 0.000000
average loss = 0.060173 h
total feature number = 5142846936
Run:{3,1,0.9}


